<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Implementing Logistic regression in Python | An Hour A Day</title>
<meta name="keywords" content="Python, Machine Learning">
<meta name="description" content="One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.
What is Logistic Regression? First of all, when we talk about Machine Learning, we are really talking about curve fitting.">
<meta name="author" content="">
<link rel="canonical" href="https://philliams.github.io/posts/machine_learning_basic/logistic_regression/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://philliams.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://philliams.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://philliams.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://philliams.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://philliams.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>

<meta property="og:title" content="Implementing Logistic regression in Python" />
<meta property="og:description" content="One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.
What is Logistic Regression? First of all, when we talk about Machine Learning, we are really talking about curve fitting." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://philliams.github.io/posts/machine_learning_basic/logistic_regression/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-01-22T12:00:00+00:00" />
<meta property="article:modified_time" content="2021-01-22T12:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Implementing Logistic regression in Python"/>
<meta name="twitter:description" content="One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.
What is Logistic Regression? First of all, when we talk about Machine Learning, we are really talking about curve fitting."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://philliams.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Implementing Logistic regression in Python",
      "item": "https://philliams.github.io/posts/machine_learning_basic/logistic_regression/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Implementing Logistic regression in Python",
  "name": "Implementing Logistic regression in Python",
  "description": "One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.\nWhat is Logistic Regression? First of all, when we talk about Machine Learning, we are really talking about curve fitting.",
  "keywords": [
    "Python", "Machine Learning"
  ],
  "articleBody": "One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.\nWhat is Logistic Regression? First of all, when we talk about Machine Learning, we are really talking about curve fitting. What this means is that we have some numerical input data as well as the numerical output we want, we’ll then use that data to create a mathmatical model that can take in some input data and output the correct values.\nIn the case of Logistic Regression, we take in a vector of numerical values, and we get an output between 0 and 1. This is a specific type of Machine Learning classification. Basically, we want to know if something about the input data is true or false, with 1 corresponding to true and 0 corresponding to false.\nNow that we’ve got that, what is Logistic Regression really? Logistic Regression is defined by two main equations:\n$$ z = \\sum w_i \\cdot x_i $$ $$ y = \\frac{1}{1+e^{-z}} $$\n$x_i$​ is the ith element of our input vector, $w_i$ is the weight of that specific input and $z$ is the weighted sum of the $x$ and $w$ vectors. $y$ on the other hand, is the final output of the Logistic Regression equation and looks like this:\nTraining a Linear Model So now we have an idea of what our model looks like and how it is defined. The next step is to actually train the model by solving for the ww vector. Assuming we have a dataset of vectors xx (all of the same size) and values yy values that we want to predict, we want to find our weight vector ww that will maximize the accuracy of our model and give correct predictions.\nThe are several algorithms that can do this, each having their own pros and cons, such as Gradient Descent or Genetic Algorithms. However, we are going to train our Logistic Regression model using Linear Regression.\nThe linear model used in Linear Regression is given by the following equation:\n$$ y = \\sum b_i \\cdot x_i $$\n$b$ is our weight vector and can be obtained via Ordinary Least Squares:\n$$ b = (X^{T}X)^{-1}(X^{T}Y) $$\nWhen solving for $b$, $X$ is a 2D matrix, each row corresponds to a single input vector, $Y$ is a vector of the known outputs for the corresponding input in the $X$ matrix, $M^T$ and $M^{-1}$ are the matrix operations (for an arbitrary matrix $M$) of transpose and inversion respectively. We will not implement these matrix functions ourselves, but will instead use the built in NumPy functions for ease.\nThis should seem very similar, since it is exactly the same equation for $z$ in the Logistic Regression model, the only difference is that we pass the sum through a sigmoid transformation when performing Logistic Regression.\nBridging Linear and Logistic Regression This is where we can use a clever trick to transform the Logistic Regression problem into a Linear Regression problem. By applying the following function to the true/false (1/0) values of the classification, we can get equivalent values to train a Linear Regression model :\n$$ y = \\frac{1}{1+e^{-z}} $$ $$ z = ln(\\frac{y}{1 - y}) $$\nHowever, if we plug in the classification values of 0 and 1, we will get a domain error when calculating $z$, since we can’t divide by 0 or calculate the log of 0. To solve this, we can simply use values very close to 0 and 1 for our classification output, for example 0.001 and 0.999. This technique is called Label Smoothing.\nSo now, to train our Logistic Regression model, we take the classification output of 1 or 0, add some small constant to avoid numerical errors, train a Linear Regression model on the transformed data, then use the Linear Model and the Logistic function to make predictions on new data.\nRecap of the algorithm Here is a recap of the algorithm to implement Logistic Regression, assuming you have a collection of numerical input vectors and the desired true/false output label:\nUse label smoothing to convert each 0/1 label into 0.001/0.999 to avoid numerical issues.\nConvert the smoothed labels into the linear domain using the following equation, where $y$ is the smoothed label and $z$ is the linear value by using: $z=ln(\\frac{y}{1−y})$\nSolve for the $b$ vector using: $ b = (X^{T}X)^{-1}(X^{T}Y) $, where the $Y$ vector is a list of the smoothed linear $z$ value\nUse the weight vector $b$ and a new input vector $x$ to make a prediction using : $$ z(x) = \\sum x_i \\cdot b_i $$ $$y(x) = \\frac{1}{1 + e^{-z(x)}}$$\nPython Implementation Now that all the of the theoretical equations have been established, we can actually implement our model and test it on some real world data. For this example, we will be using the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset. You can download a copy of the dataset directly, or you can import it through the Scikit learn dataset module. We are trying to predict if a tumor is bening or malignant with several features such as the radius, symmetry, smoothness and texture.\nfrom sklearn.datasets import load_breast_cancer import numpy import math # Define class to implement our logistic regression model class LogisticRegression(): def __init__(self, use_bias=False, smooth=1e-2, cutoff=0.5): # bias determines if we use a bias term or not self.use_bias = use_bias # instance variable for our weight vector self.weights = None # the cutoff is used to determine the prediction, if y(z) \u003e= cutoff, y(z) = 1, else y(z) = 0 self.cutoff = cutoff # the amount of smoothing used on the output labels self.smooth = smooth # will smooth all the labels given the Y vector def smooth_labels(self, Y): y_smooth = [] for i in range(len(Y)): if Y[i] \u003e= 1 - self.smooth: y_smooth += [1.0 - self.smooth] else: y_smooth += [self.smooth] return numpy.array(y_smooth) # calculates y given a single z def logistic_function(self, z): y = 1.0/(1.0 + math.exp(-z)) return y # calculates z given a single y def inverse_logistic_function(self, y): z = math.log(y/(1-y)) return z # convert the labels from 0/1 values to linear values def transform_labels_to_linear(self, Y): for i in range(len(Y)): Y[i] = self.inverse_logistic_function(Y[i]) return Y # use the weights and a new vector to make a prediction def predict_on_vector(self, x): # calculate the weighted sum z = numpy.matmul(numpy.transpose(x), self.weights) prediction = self.logistic_function(z) if prediction \u003e= self.cutoff: return 1 else: return 0 def predict(self, X): # using a bias will add a feature to each vector that is set to 1 # this allows the model to learn a \"default\" value from this constant # the bias can be thought of as the offset, while the weights are the slopes if self.use_bias: ones = numpy.array([[1.0] for i in range(len(X))]) X = numpy.append(ones, X, axis=1) # calculate the prediction for each vector predictions = [] for i in range(len(X)): prediction = self.predict_on_vector(X[i]) predictions += [prediction] return numpy.array(predictions) # train the model on the dataset def train(self, X, Y): # using a bias will add a feature to each vector that is set to 1 # this allows the model to learn a \"default\" value from this constant # the bias can be thought of as the offset, while the weights are the slopes if self.use_bias: ones = numpy.array([[1.0] for i in range(len(X))]) X = numpy.append(ones, X, axis=1) # smooth the labels Y = self.smooth_labels(Y) # convert labels to linear Z = self.transform_labels_to_linear(Y) # calculate weights self.weights = numpy.matmul( numpy.linalg.inv( numpy.matmul( numpy.transpose(X), X ) ), numpy.matmul( numpy.transpose(X), Y ) ) # Apply the logistic regression model to the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset X, Y = load_breast_cancer(return_X_y=True) #split the data into training and testing sets X_test = X[:100] X_train = X[100:] Y_test = Y[:100] Y_train = Y[100:] # Initialize the model model = LogisticRegression(use_bias=True) # Train the model on the training set model.train(X_train, Y_train) # calculate the accuracy on the training set predictions = model.predict(X_train) print(\"Training Accuracy (%) = \" + str(100 * numpy.sum(predictions == Y_train)/ float(len(Y_train)))) # calculate the accuracy on the testing set predictions = model.predict(X_test) print(\"Testing Accuracy (%) = \" + str(100 * numpy.sum(predictions == Y_test)/ float(len(Y_test)))) ",
  "wordCount" : "1401",
  "inLanguage": "en",
  "datePublished": "2021-01-22T12:00:00Z",
  "dateModified": "2021-01-22T12:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://philliams.github.io/posts/machine_learning_basic/logistic_regression/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "An Hour A Day",
    "logo": {
      "@type": "ImageObject",
      "url": "https://philliams.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://philliams.github.io/" accesskey="h" title="An Hour A Day (Alt + H)">An Hour A Day</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://philliams.github.io/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="https://philliams.github.io/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="https://philliams.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://philliams.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://philliams.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Implementing Logistic regression in Python
    </h1>
    <div class="post-meta"><span title='2021-01-22 12:00:00 +0000 UTC'>January 22, 2021</span>&nbsp;·&nbsp;7 min

</div>
  </header> 
  <div class="post-content"><p>One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.</p>
<h2 id="what-is-logistic-regression">What is Logistic Regression?<a hidden class="anchor" aria-hidden="true" href="#what-is-logistic-regression">#</a></h2>
<p>First of all, when we talk about Machine Learning, we are really talking about curve fitting. What this means is that we have some numerical input data as well as the numerical output we want, we’ll then use that data to create a mathmatical model that can take in some input data and output the correct values.</p>
<p>In the case of Logistic Regression, we take in a vector of numerical values, and we get an output between 0 and 1. This is a specific type of Machine Learning classification. Basically, we want to know if something about the input data is true or false, with 1 corresponding to true and 0 corresponding to false.</p>
<p>Now that we’ve got that, what is Logistic Regression really? Logistic Regression is defined by two main equations:</p>
<p>$$ z = \sum w_i \cdot x_i $$
$$ y = \frac{1}{1+e^{-z}} $$</p>
<p>$x_i$​ is the ith element of our input vector, $w_i$ is the weight of that specific input and $z$ is the weighted sum of the $x$ and $w$ vectors. $y$ on the other hand, is the final output of the Logistic Regression equation and looks like this:</p>
<iframe src="https://www.desmos.com/calculator/cyrjcwodjq?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0>
</iframe>
<h2 id="training-a-linear-model">Training a Linear Model<a hidden class="anchor" aria-hidden="true" href="#training-a-linear-model">#</a></h2>
<p>So now we have an idea of what our model looks like and how it is defined. The next step is to actually train the model by solving for the ww vector. Assuming we have a dataset of vectors xx (all of the same size) and values yy values that we want to predict, we want to find our weight vector ww that will maximize the accuracy of our model and give correct predictions.</p>
<p>The are several algorithms that can do this, each having their own pros and cons, such as Gradient Descent or Genetic Algorithms. However, we are going to train our Logistic Regression model using Linear Regression.</p>
<p>The linear model used in Linear Regression is given by the following equation:</p>
<p>$$ y = \sum b_i \cdot x_i $$</p>
<p>$b$ is our weight vector and can be obtained via Ordinary Least Squares:</p>
<p>$$ b = (X^{T}X)^{-1}(X^{T}Y) $$</p>
<p>When solving for $b$, $X$ is a 2D matrix, each row corresponds to a single input vector, $Y$ is a vector of the known outputs for the corresponding input in the $X$ matrix, $M^T$ and $M^{-1}$ are the matrix operations (for an arbitrary matrix $M$) of transpose and inversion respectively. We will not implement these matrix functions ourselves, but will instead use the built in NumPy functions for ease.</p>
<p>This should seem very similar, since it is exactly the same equation for $z$ in the Logistic Regression model, the only difference is that we pass the sum through a sigmoid transformation when performing Logistic Regression.</p>
<h2 id="bridging-linear-and-logistic-regression">Bridging Linear and Logistic Regression<a hidden class="anchor" aria-hidden="true" href="#bridging-linear-and-logistic-regression">#</a></h2>
<p>This is where we can use a clever trick to transform the Logistic Regression problem into a Linear Regression problem. By applying the following function to the true/false (1/0) values of the classification, we can get equivalent values to train a Linear Regression model :</p>
<p>$$ y = \frac{1}{1+e^{-z}} $$
$$ z = ln(\frac{y}{1 - y}) $$</p>
<p>However, if we plug in the classification values of 0 and 1, we will get a domain error when calculating $z$, since we can’t divide by 0 or calculate the log of 0. To solve this, we can simply use values very close to 0 and 1 for our classification output, for example 0.001 and 0.999. This technique is called Label Smoothing.</p>
<p>So now, to train our Logistic Regression model, we take the classification output of 1 or 0, add some small constant to avoid numerical errors, train a Linear Regression model on the transformed data, then use the Linear Model and the Logistic function to make predictions on new data.</p>
<h2 id="recap-of-the-algorithm">Recap of the algorithm<a hidden class="anchor" aria-hidden="true" href="#recap-of-the-algorithm">#</a></h2>
<p>Here is a recap of the algorithm to implement Logistic Regression, assuming you have a collection of numerical input vectors and the desired true/false output label:</p>
<ol>
<li>
<p>Use label smoothing to convert each 0/1 label into 0.001/0.999 to avoid numerical issues.</p>
</li>
<li>
<p>Convert the smoothed labels into the linear domain using the following equation, where $y$ is the smoothed label and $z$ is the linear value by using: $z=ln(\frac{y}{1−y})$</p>
</li>
<li>
<p>Solve for the $b$ vector using: $ b = (X^{T}X)^{-1}(X^{T}Y) $, where the $Y$ vector is a list of the smoothed linear $z$ value</p>
</li>
<li>
<p>Use the weight vector $b$ and a new input vector $x$ to make a prediction using :
$$ z(x) = \sum x_i \cdot b_i $$
$$y(x) = \frac{1}{1 + e^{-z(x)}}$$</p>
</li>
</ol>
<h2 id="python-implementation">Python Implementation<a hidden class="anchor" aria-hidden="true" href="#python-implementation">#</a></h2>
<p>Now that all the of the theoretical equations have been established, we can actually implement our model and test it on some real world data. For this example, we will be using the <a href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic">UCI ML Breast Cancer Wisconsin (Diagnostic) dataset</a>. You can download a copy of the dataset directly, or you can import it through the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html">Scikit learn dataset module</a>. We are trying to predict if a tumor is bening or malignant with several features such as the radius, symmetry, smoothness and texture.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_breast_cancer
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define class to implement our logistic regression model</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LogisticRegression</span>():
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> __init__(self, use_bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, smooth<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-2</span>, cutoff<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>):
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># bias determines if we use a bias term or not</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>use_bias <span style="color:#f92672">=</span> use_bias
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># instance variable for our weight vector</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>weights <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># the cutoff is used to determine the prediction, if y(z) &gt;= cutoff, y(z) = 1, else y(z) = 0</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>cutoff <span style="color:#f92672">=</span> cutoff
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># the amount of smoothing used on the output labels</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>smooth <span style="color:#f92672">=</span> smooth
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># will smooth all the labels given the Y vector</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">smooth_labels</span>(self, Y):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		y_smooth <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(Y)):
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">if</span> Y[i] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>smooth:
</span></span><span style="display:flex;"><span>				y_smooth <span style="color:#f92672">+=</span> [<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>smooth]
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>				y_smooth <span style="color:#f92672">+=</span> [self<span style="color:#f92672">.</span>smooth]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> numpy<span style="color:#f92672">.</span>array(y_smooth)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># calculates y given a single z</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_function</span>(self, z):
</span></span><span style="display:flex;"><span>		y <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> math<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>z))
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># calculates z given a single y</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">inverse_logistic_function</span>(self, y):
</span></span><span style="display:flex;"><span>		z <span style="color:#f92672">=</span> math<span style="color:#f92672">.</span>log(y<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>y))
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> z
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># convert the labels from 0/1 values to linear values</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transform_labels_to_linear</span>(self, Y):
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(Y)):
</span></span><span style="display:flex;"><span>			Y[i] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>inverse_logistic_function(Y[i])
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> Y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># use the weights and a new vector to make a prediction</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_on_vector</span>(self, x):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># calculate the weighted sum</span>
</span></span><span style="display:flex;"><span>		z <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>matmul(numpy<span style="color:#f92672">.</span>transpose(x), self<span style="color:#f92672">.</span>weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		prediction <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>logistic_function(z)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> prediction <span style="color:#f92672">&gt;=</span> self<span style="color:#f92672">.</span>cutoff:
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># using a bias will add a feature to each vector that is set to 1</span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># this allows the model to learn a &#34;default&#34; value from this constant</span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># the bias can be thought of as the offset, while the weights are the slopes</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>use_bias:
</span></span><span style="display:flex;"><span>			ones <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1.0</span>] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X))])
</span></span><span style="display:flex;"><span>			X <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>append(ones, X, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># calculate the prediction for each vector</span>
</span></span><span style="display:flex;"><span>		predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X)):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			prediction <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict_on_vector(X[i])
</span></span><span style="display:flex;"><span>			predictions <span style="color:#f92672">+=</span> [prediction]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> numpy<span style="color:#f92672">.</span>array(predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># train the model on the dataset</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(self, X, Y):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># using a bias will add a feature to each vector that is set to 1</span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># this allows the model to learn a &#34;default&#34; value from this constant</span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># the bias can be thought of as the offset, while the weights are the slopes</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>use_bias:
</span></span><span style="display:flex;"><span>			ones <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1.0</span>] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X))])
</span></span><span style="display:flex;"><span>			X <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>append(ones, X, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># smooth the labels</span>
</span></span><span style="display:flex;"><span>		Y <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>smooth_labels(Y)
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># convert labels to linear</span>
</span></span><span style="display:flex;"><span>		Z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transform_labels_to_linear(Y)
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># calculate weights</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>weights <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>matmul(
</span></span><span style="display:flex;"><span>				numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(
</span></span><span style="display:flex;"><span>					numpy<span style="color:#f92672">.</span>matmul(
</span></span><span style="display:flex;"><span>						numpy<span style="color:#f92672">.</span>transpose(X),
</span></span><span style="display:flex;"><span>						X
</span></span><span style="display:flex;"><span>					)
</span></span><span style="display:flex;"><span>				),
</span></span><span style="display:flex;"><span>				numpy<span style="color:#f92672">.</span>matmul(
</span></span><span style="display:flex;"><span>					numpy<span style="color:#f92672">.</span>transpose(X),
</span></span><span style="display:flex;"><span>					Y
</span></span><span style="display:flex;"><span>				)
</span></span><span style="display:flex;"><span>			)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Apply the logistic regression model to the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset</span>
</span></span><span style="display:flex;"><span>X, Y <span style="color:#f92672">=</span> load_breast_cancer(return_X_y<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#split the data into training and testing sets</span>
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> X[:<span style="color:#ae81ff">100</span>]
</span></span><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> X[<span style="color:#ae81ff">100</span>:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Y_test <span style="color:#f92672">=</span> Y[:<span style="color:#ae81ff">100</span>]
</span></span><span style="display:flex;"><span>Y_train <span style="color:#f92672">=</span> Y[<span style="color:#ae81ff">100</span>:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize the model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> LogisticRegression(use_bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train the model on the training set</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>train(X_train, Y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># calculate the accuracy on the training set</span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_train)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Training Accuracy (%) = &#34;</span> <span style="color:#f92672">+</span> str(<span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> numpy<span style="color:#f92672">.</span>sum(predictions <span style="color:#f92672">==</span> Y_train)<span style="color:#f92672">/</span> float(len(Y_train))))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># calculate the accuracy on the testing set</span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Testing Accuracy (%) = &#34;</span> <span style="color:#f92672">+</span> str(<span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> numpy<span style="color:#f92672">.</span>sum(predictions <span style="color:#f92672">==</span> Y_test)<span style="color:#f92672">/</span> float(len(Y_test))))
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://philliams.github.io/tags/python/">Python</a></li>
      <li><a href="https://philliams.github.io/tags/machine-learning/">Machine Learning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://philliams.github.io/">An Hour A Day</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
