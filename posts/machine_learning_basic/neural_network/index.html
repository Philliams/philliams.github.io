<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Implementing Neural Networks in Python | An Hour A Day</title>
<meta name="keywords" content="Python, Machine Learning">
<meta name="description" content="One of the more interesting Machine Learning models is the Neural Network. A Neural Network is a highly non-linear mathematical model that can be fitted to very complicated datasets, from image classification to text translation. In this blog post, we’ll be implementing our own simple Neural Network library in python, then test how our model performs through a practical example on an image classification dataset.
What is a Neural Network? There are several types of Neural Networks, however we will be examining the simplest variant : a simple Feed-Forward Neural Network.">
<meta name="author" content="">
<link rel="canonical" href="https://philliams.github.io/posts/machine_learning_basic/neural_network/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://philliams.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://philliams.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://philliams.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://philliams.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://philliams.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>

<meta property="og:title" content="Implementing Neural Networks in Python" />
<meta property="og:description" content="One of the more interesting Machine Learning models is the Neural Network. A Neural Network is a highly non-linear mathematical model that can be fitted to very complicated datasets, from image classification to text translation. In this blog post, we’ll be implementing our own simple Neural Network library in python, then test how our model performs through a practical example on an image classification dataset.
What is a Neural Network? There are several types of Neural Networks, however we will be examining the simplest variant : a simple Feed-Forward Neural Network." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://philliams.github.io/posts/machine_learning_basic/neural_network/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-01-23T12:00:00+00:00" />
<meta property="article:modified_time" content="2021-01-23T12:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Implementing Neural Networks in Python"/>
<meta name="twitter:description" content="One of the more interesting Machine Learning models is the Neural Network. A Neural Network is a highly non-linear mathematical model that can be fitted to very complicated datasets, from image classification to text translation. In this blog post, we’ll be implementing our own simple Neural Network library in python, then test how our model performs through a practical example on an image classification dataset.
What is a Neural Network? There are several types of Neural Networks, however we will be examining the simplest variant : a simple Feed-Forward Neural Network."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://philliams.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Implementing Neural Networks in Python",
      "item": "https://philliams.github.io/posts/machine_learning_basic/neural_network/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Implementing Neural Networks in Python",
  "name": "Implementing Neural Networks in Python",
  "description": "One of the more interesting Machine Learning models is the Neural Network. A Neural Network is a highly non-linear mathematical model that can be fitted to very complicated datasets, from image classification to text translation. In this blog post, we’ll be implementing our own simple Neural Network library in python, then test how our model performs through a practical example on an image classification dataset.\nWhat is a Neural Network? There are several types of Neural Networks, however we will be examining the simplest variant : a simple Feed-Forward Neural Network.",
  "keywords": [
    "Python", "Machine Learning"
  ],
  "articleBody": "One of the more interesting Machine Learning models is the Neural Network. A Neural Network is a highly non-linear mathematical model that can be fitted to very complicated datasets, from image classification to text translation. In this blog post, we’ll be implementing our own simple Neural Network library in python, then test how our model performs through a practical example on an image classification dataset.\nWhat is a Neural Network? There are several types of Neural Networks, however we will be examining the simplest variant : a simple Feed-Forward Neural Network. In a Feed-Forward Neural Network, there are 3 main components :\nNeurons Activation Functions Layers A neuron is a simple non-linear entity, it can be defined by the two following equations :\n$$ z = b + \\sum x_i \\cdot w_i $$\n$$ y = f(z) $$\nIn this notation, the $x_i$ value indicates the ith value of the input vector $x$, while the $w_i$ value is the ith value of the weight vector $w$. The $b$ term is called the bias and and allows the neuron to model an offset, while the weights allow the neuron to model how quickly the output should change in response to a change in the inputs. Finally, the $f(z)$ function is an arbitrary non-linear function typically chosen from a catalogue of standard functions.\nActivation Functions The output of the neuron is defined as some function $f(z)$ of the weighted sum, but what is is it? $f(z)$ is called the activation function and can be chosen from a selection of commonly used functions (or you can implement your own custom function as long as it is continuous and differentiable). Some common activations functions are the Sigmoid function (also called logistic function), the Hyperbolic Tangent function or the Rectifier function (also called ReLu).\nThe Sigmoid function is given by:\n$$ f(z) = \\frac{1}{1+e^{-z}} $$\nThe Hyperbolic Tangent function is defined as follows :\n$$ f(z) = \\frac{e^{2z} - 1}{e^{2z} + 1} $$\nThe Rectifier function (ReLu) is given as follows:\n$$ f(z) = max(z, 0) $$\nAn interesting fact about the Rectifier Function is that the derivative is not defined at 0, since the function is piecewise, however in practice we can simply decide what side of the piecewise function to use when calculating the derivative.\nNow that we have the defining equations for the neurons and the activation functions, we can arrange the neurons into layers to form an actual Neural Network. A Neural Network can be made up of a single neuron or thousands of them, but typically conforms to the following architecture :\nIn the diagram above, the input layer represents the raw numerical input vector, each circle in the input layer represents a single input dimension, so if there is 16 circles in the input layer, we can imagine that the neural network will receive a vector of 16 values as input. We call these input values features.\nThe lines represent the connections between the neurons, so if there is a line between two neurons, the output of the neuron on the left will be taken as an input to the neuron on the right.\nFor the hidden and output layers, each circle represents a single neuron, and as you can see, all the outputs from one layer are fed in as inputs to all the neurons in the next layer.\nThe number of layers, and the number of neurons in each layer is determined experimentally or inspired by heuristics, however the input and output layer sizes are usually determined by the available data. For example, you may try to predict a set of 3 values based on an input vector with 32 features.\nWith the structure of the Neural Network defined, and the functions within the neurons given, we can establish how to actually solve for the weights that will give the best performance.\nGradient Descent and Backpropagation There are two algorithms commonly used to solve for the weights of a neural network: Gradient Descent and Backpropagation. First let’s look at the output neurons of the network.\nTo understand Gradient Descent, we need to choose a cost function $C$ (also called error $E$). This cost function should decrease as performance gets better, this way we can frame the problem of training a Neural Network as optimizing the weights of each neuron in the network to minimize the cost (or error) of the network on some training data. For our network we will be using the mean squared error (MSE), which is defined as follows :\n$$ E = \\frac{1}{2}(y - \\hat{y})^2 $$\nThe further away we are from the right answer, the larger our error $E$ is. Note that $y$ is the true value for a given data point and $\\hat{y}$​ is the output of our model for that same input. We can now find the derivative of the error with regard to the model output, so that we can figure out how the error changes with the output of the model:\n$$ \\frac{\\partial E}{\\partial \\hat{y}} = y - \\hat{y}$$\nBut we what we really want to know is how the cost changes with a change in the weights, since that would allow us to optimize the weights directly. The first step is to go one step further with our derivative (using the chain rule) and find how $E$ changes with the $z$ value of the output neuron:\n$$ \\frac{\\partial E}{\\partial z} = \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} $$\nFinally, we can go one step further and derive the error such that we get the derivative of the error relative to the weight of the output neuron:\n$$ \\frac{\\partial E}{\\partial w_i} = \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i} $$\nBut from earlier, we know that $\\hat{y}=f(z)$ and $ z = b +\\sum w_i \\cdot x_i $​, so we by replacing the partial derivatives in the previous equation, we can get the actual derivative of the error relative to the weights and bias:\n$$ \\frac{\\partial E}{\\partial w_i} = (y - \\hat{y}) \\cdot f’(z) \\cdot x_i$$ $$ \\frac{\\partial E}{\\partial b} = (y - \\hat{y}) \\cdot f’(z) $$\nSo now that we have the derivative, what do we do? Here, we apply Gradient Descent. Basically, using the derivative we can figure out if an increase or a decrease of the value of the weight will decrease the cost, then we iteratively update the weights to minimize the weight. $\\eta$ is called the “Learning Rate” and is a constant chosen to adjust the size of the iteration steps. By iteratively updating the weights and biases, we can converge to a local minima which will (hopefully) give us a model with a good prediction accuracy:\n$$ w_{i,t+1} = w_{i, t} + \\eta \\cdot \\frac{\\partial E}{\\partial w_{i, t}}$$ $$ b_{t+1} = b_{t} + \\eta \\cdot \\frac{\\partial E}{\\partial b_{t}}$$\nHowever, in the hidden layers, we don’t know what the desired output of the specific neurone is, this is where backpropagation comes in handy. Basically, we will propagate the errors of the output layer back through the network so that we can calculate the derivatives of the weights relative to the cost for all of the weights.\nFor this, we will need to define another equations. We replace the $\\frac{\\partial E}{\\partial \\hat{y}}$ with:\n$$ \\frac{\\partial E}{\\partial \\hat{y}} = \\sum w_i \\cdot \\frac{\\partial E}{\\partial z_i}$$\nThis tells us that for a node in a hidden layer, the derivative of the error relative to it’s weights is equal to the weighted sums of the derivatives of the next layer. The weights used are simply the weights of the nodes in the next layer for that given node.\nWe can apply all the same equations as before when calculating the derivatives as before, we simply need to calculate the initial $\\frac{\\partial E}{\\partial \\hat{y}}$​ of the output layer, then use the derivative equations to propagate the derivatives through the network, then finally update the weights after we’ve calculated the relevant values.\nRecap of the algorithm Here is a recap of the algorithm to implement a simple Neural Network, assuming you have a collection of numerical input vectors and the desired true/false output label:\nCalculate the derivatives of the output neurons relative to the error using an input vector $x$ with a known output $y$ $$ \\frac{\\partial E}{\\partial w_i} = (y - \\hat{y}) \\cdot f’(z) \\cdot x_i$$ $$ \\frac{\\partial E}{\\partial b} = (y - \\hat{y}) \\cdot f’(z) $$\nPropagate the errors of the last layer to the second last layer, then propagate the errors of the second last layer to the third last layer, etc. where $\\hat{y}$​ is the output of a hidden node $$ \\frac{\\partial E}{\\partial \\hat{y}} = \\sum w_i \\cdot \\frac{\\partial E}{\\partial z_i}$$\nOnce you know the derivatives of all the weights and biases for all the neurons, update them using the following rules: $$ w_{i,t+1} = w_{i, t} + \\eta \\cdot \\frac{\\partial E}{\\partial w_{i, t}}$$ $$ b_{t+1} = b_{t} + \\eta \\cdot \\frac{\\partial E}{\\partial b_{t}}$$\nPython Implementation Now that all the of the theoretical equations have been established, we can actually implement our model and test it on some real world data. For this example, we will be using the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset. You can download a copy of the dataset directly, or you can import it through the Scikit learn dataset module. We are trying to predict if a tumor is bening or malignant with several features such as the radius, symmetry, smoothness and texture.\nfrom sklearn.datasets import load_breast_cancer import numpy import math import random # implementation of sigmoid neurones in a layer class SigmoidLayer: # layer is defined by the input size and the number of neurones def __init__(self, input_size, number_of_neurones): self.input_size = input_size self.number_of_neurones = number_of_neurones # generate the weights as random numpy arrays of small values # different initiliaztion schemes can be used for the weights with varying experimental success self.biases = numpy.random.normal(0., 0.1, number_of_neurones) self.weights = numpy.random.normal(0., 0.1, (number_of_neurones, input_size)) # defined the sigmoid function def sigmoid(self, z): return 1.0/(1.0 + math.exp(-z)) # calculate the output of the neurones as a vector, based on an input vector to the layer def output(self, x): # initialize layer output, output will be of length self.number_of_neurones output_vector = [] # iterate over each neurone in the layer for neurone in range(self.number_of_neurones): # retrieve the bias term for the given neurone z = self.biases[neurone] # calculate the weighted sum and add it to the initial bias term for feature in range(self.input_size): z += self.weights[neurone][feature] * x[feature] # apply the activation function neurone_output = self.sigmoid(z) # add the output of the given neurone to the layer output output_vector += [neurone_output] # return the output of the layer as a vector return numpy.array(output_vector) # calculate the backpropagation step for the layer as well as update the weights, based on the last input, output # and the derivatives of the layer, will output the derivatives needed for the previous layer def backpropagate(self, last_input, last_output, dE_dy, learning_rate): dE_dz = [] # use the dE_dy of the layer to calculate the dE_dz of each neurone in the layer for neurone in range(self.number_of_neurones): # apply the derivative of the sigmoid function neurone_dE_dz = last_output[neurone] * (1.0 - last_output[neurone]) * dE_dy[neurone] # keep track of the derivatives of each neurone dE_dz += [neurone_dE_dz] dE_dz = numpy.array(dE_dz) # use the dE_dz derivative as well as the last input to update the weights and biases # this is the gradient descent step for neurone in range(self.number_of_neurones): # update the bias of each neurone in the layer self.biases[neurone] -= learning_rate * dE_dz[neurone] for feature in range(self.input_size): # calculate the derivative relative to each weight, then update each weight for each neurone self.weights[neurone][feature] -= learning_rate * last_input[feature] * dE_dz[neurone] # calculate the dE_dy derivative to be used by the following layer next_layer_dE_dy = numpy.zeros(self.input_size) # iterate over each neurone for neurone in range(self.number_of_neurones): # iterate over each weight for feature in range(self.input_size): # calculate the derivative using the backpropagation rule next_layer_dE_dy[feature] += self.weights[neurone][feature] * dE_dz[neurone] return next_layer_dE_dy # implement Neural Network using sigmoid layer class SigmoidFeedForwardNeuralNetwork: # we need the number and sizes of the layers, the input size and the learning rate for the network def __init__(self, learning_rate, input_size, layer_sizes): self.learning_rate = learning_rate self.layers = None # initialize each layer based on the defined sizes for layer in range(len(layer_sizes)): # input size of first layer is input size if layer == 0: self.layers = [SigmoidLayer(input_size, layer_sizes[layer])] # input size of every other layer is the size of the previous layer else: self.layers += [SigmoidLayer(layer_sizes[layer - 1], layer_sizes[layer])] # calculate the output of the neural network for a given input layer def predict_on_vector(self, x): # feed the output of each layer as the input to the next layer for layer in self.layers: x = layer.output(x) # return the output of the last layer return x # calculate the outputs of the neural network for a list of vectors def predict(self, X): # calculate the prediction for each vector predictions = [] # make a prediction for each vector in the set for i in range(len(X)): prediction = self.predict_on_vector(X[i]) predictions += [prediction] # return all of the predictions return numpy.array(predictions) def train(self, X, Y, iterations=1): # generate a list of indexes for the training samples training_samples = [i for i in range(len(X))] # do k iterations of training on the dataset for k in range(iterations): # print training progress print(\"Epoch : \" + str(k)) # randomly shuffle dataset to avoid getting stuck in local minima # random.shuffle(training_samples) # train on each sample for index in training_samples: self.train_on_vector(X[index], Y[index]) # train the Neural Network on a single vector, this training scheme is know as \"on-line\" training # as the neural network is updated every time a new vector is given def train_on_vector(self, x, y): outputs = [] inputs = [] # iterate over each layer and keep track of the inputs and output for layer in self.layers: inputs += [x] x = layer.output(x) outputs += [x] # calculate the error vector of the output neurones error_vector = x - y # iterate over each layer and apply the backpropagation, starting for i in range(len(self.layers)): index = len(self.layers) - 1 - i # update the weights of the layers and retrieve the next layer's error vector error_vector = self.layers[index].backpropagate(inputs[index], outputs[index], error_vector, self.learning_rate) # Apply the logistic regression model to the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset X, Y = load_breast_cancer(return_X_y=True) Y = [numpy.array([label]) for label in Y] #split the data into training and testing sets X_test = X[:100] X_train = X[100:] Y_test = Y[:100] Y_train = Y[100:] # Initialize the model model = SigmoidFeedForwardNeuralNetwork(1e-1, 2, [64, 8, 1]) # Train the model on the training set model.train(X_train, Y_train, iterations=25) # calculate the accuracy on the training set predictions = model.predict(X_train) accuracy = 0 for i in range(len(predictions)): if predictions[i][0] \u003e= 0.5 and Y_train[i][0] == 1: accuracy += 1 elif predictions[i][0] \u003c 0.5 and Y_train[i][0] == 0: accuracy += 1 print(\"Training Accuracy (%) = \" + str(100. * accuracy / float(len(predictions)))) # calculate the accuracy on the testing set predictions = model.predict(X_test) accuracy = 0 for i in range(len(predictions)): if predictions[i][0] \u003e= 0.5 and Y_test[i][0] == 1: accuracy += 1 elif predictions[i][0] \u003c 0.5 and Y_test[i][0] == 0: accuracy += 1 print(\"Testing Accuracy (%) = \" + str(100. * accuracy / float(len(predictions)))) ",
  "wordCount" : "2551",
  "inLanguage": "en",
  "datePublished": "2021-01-23T12:00:00Z",
  "dateModified": "2021-01-23T12:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://philliams.github.io/posts/machine_learning_basic/neural_network/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "An Hour A Day",
    "logo": {
      "@type": "ImageObject",
      "url": "https://philliams.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://philliams.github.io/" accesskey="h" title="An Hour A Day (Alt + H)">An Hour A Day</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://philliams.github.io/publications/" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="https://philliams.github.io/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="https://philliams.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://philliams.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://philliams.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Implementing Neural Networks in Python
    </h1>
    <div class="post-meta"><span title='2021-01-23 12:00:00 +0000 UTC'>January 23, 2021</span>&nbsp;·&nbsp;12 min

</div>
  </header> 
  <div class="post-content"><p>One of the more interesting Machine Learning models is the Neural Network. A Neural Network is a highly non-linear mathematical model that can be fitted to very complicated datasets, from image classification to text translation. In this blog post, we’ll be implementing our own simple Neural Network library in python, then test how our model performs through a practical example on an image classification dataset.</p>
<h2 id="what-is-a-neural-network">What is a Neural Network?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-neural-network">#</a></h2>
<p>There are several types of Neural Networks, however we will be examining the simplest variant : a simple Feed-Forward Neural Network. In a Feed-Forward Neural Network, there are 3 main components :</p>
<ol>
<li>Neurons</li>
<li>Activation Functions</li>
<li>Layers</li>
</ol>
<p>A neuron is a simple non-linear entity, it can be defined by the two following equations :</p>
<p>$$ z = b + \sum x_i \cdot w_i $$</p>
<p>$$ y = f(z) $$</p>
<p>In this notation, the $x_i$ value indicates the ith value of the input vector $x$, while the $w_i$ value is the ith value of the weight vector $w$. The $b$ term is called the bias and and allows the neuron to model an offset, while the weights allow the neuron to model how quickly the output should change in response to a change in the inputs. Finally, the $f(z)$ function is an arbitrary non-linear function typically chosen from a catalogue of standard functions.</p>
<h2 id="activation-functions">Activation Functions<a hidden class="anchor" aria-hidden="true" href="#activation-functions">#</a></h2>
<p>The output of the neuron is defined as some function $f(z)$ of the weighted sum, but what is is it? $f(z)$ is called the activation function and can be chosen from a selection of commonly used functions (or you can implement your own custom function as long as it is continuous and differentiable). Some common activations functions are the Sigmoid function (also called logistic function), the Hyperbolic Tangent function or the Rectifier function (also called ReLu).</p>
<p>The Sigmoid function is given by:</p>
<p>$$ f(z) = \frac{1}{1+e^{-z}} $$</p>
<iframe src="https://www.desmos.com/calculator/3xmoswokp1?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>
<p>The Hyperbolic Tangent function is defined as follows :</p>
<p>$$ f(z) = \frac{e^{2z} - 1}{e^{2z} + 1} $$</p>
<iframe src="https://www.desmos.com/calculator/mvvxv4jcjp?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>
<p>The Rectifier function (ReLu) is given as follows:</p>
<p>$$ f(z) = max(z, 0) $$</p>
<iframe src="https://www.desmos.com/calculator/n2jbzumgki?embed" width="500" height="500" style="border: 1px solid #ccc" frameborder=0></iframe>
<p>An interesting fact about the Rectifier Function is that the derivative is not defined at 0, since the function is piecewise, however in practice we can simply decide what side of the piecewise function to use when calculating the derivative.</p>
<p>Now that we have the defining equations for the neurons and the activation functions, we can arrange the neurons into layers to form an actual Neural Network. A Neural Network can be made up of a single neuron or thousands of them, but typically conforms to the following architecture :</p>
<p><img loading="lazy" src="/posts/neural_network/neural_network.svg" alt="Neural Network Architecture"  />
</p>
<p>In the diagram above, the input layer represents the raw numerical input vector, each circle in the input layer represents a single input dimension, so if there is 16 circles in the input layer, we can imagine that the neural network will receive a vector of 16 values as input. We call these input values features.</p>
<p>The lines represent the connections between the neurons, so if there is a line between two neurons, the output of the neuron on the left will be taken as an input to the neuron on the right.</p>
<p>For the hidden and output layers, each circle represents a single neuron, and as you can see, all the outputs from one layer are fed in as inputs to all the neurons in the next layer.</p>
<p>The number of layers, and the number of neurons in each layer is determined experimentally or inspired by heuristics, however the input and output layer sizes are usually determined by the available data. For example, you may try to predict a set of 3 values based on an input vector with 32 features.</p>
<p>With the structure of the Neural Network defined, and the functions within the neurons given, we can establish how to actually solve for the weights that will give the best performance.</p>
<h2 id="gradient-descent-and-backpropagation">Gradient Descent and Backpropagation<a hidden class="anchor" aria-hidden="true" href="#gradient-descent-and-backpropagation">#</a></h2>
<p>There are two algorithms commonly used to solve for the weights of a neural network: Gradient Descent and Backpropagation. First let’s look at the output neurons of the network.</p>
<p>To understand Gradient Descent, we need to choose a cost function $C$ (also called error $E$). This cost function should decrease as performance gets better, this way we can frame the problem of training a Neural Network as optimizing the weights of each neuron in the network to minimize the cost (or error) of the network on some training data. For our network we will be using the mean squared error (MSE), which is defined as follows :</p>
<p>$$ E = \frac{1}{2}(y - \hat{y})^2 $$</p>
<p>The further away we are from the right answer, the larger our error $E$ is. Note that $y$ is the true value for a given data point and $\hat{y}$​ is the output of our model for that same input. We can now find the derivative of the error with regard to the model output, so that we can figure out how the error changes with the output of the model:</p>
<p>$$ \frac{\partial E}{\partial \hat{y}} = y - \hat{y}$$</p>
<p>But we what we really want to know is how the cost changes with a change in the weights, since that would allow us to optimize the weights directly. The first step is to go one step further with our derivative (using the chain rule) and find how $E$ changes with the $z$ value of the output neuron:</p>
<p>$$ \frac{\partial E}{\partial z} = \frac{\partial E}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} $$</p>
<p>Finally, we can go one step further and derive the error such that we get the derivative of the error relative to the weight of the output neuron:</p>
<p>$$ \frac{\partial E}{\partial w_i} = \frac{\partial E}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w_i} $$</p>
<p>But from earlier, we know that $\hat{y}=f(z)$ and $ z = b +\sum w_i \cdot x_i $​, so we by replacing the partial derivatives in the previous equation, we can get the actual derivative of the error relative to the weights and bias:</p>
<p>$$ \frac{\partial E}{\partial w_i} = (y - \hat{y}) \cdot f&rsquo;(z) \cdot x_i$$
$$ \frac{\partial E}{\partial b} = (y - \hat{y}) \cdot f&rsquo;(z) $$</p>
<p>So now that we have the derivative, what do we do? Here, we apply Gradient Descent. Basically, using the derivative we can figure out if an increase or a decrease of the value of the weight will decrease the cost, then we iteratively update the weights to minimize the weight. $\eta$ is called the &ldquo;Learning Rate&rdquo; and is a constant chosen to adjust the size of the iteration steps. By iteratively updating the weights and biases, we can converge to a local minima which will (hopefully) give us a model with a good prediction accuracy:</p>
<p>$$ w_{i,t+1} = w_{i, t} + \eta \cdot \frac{\partial E}{\partial w_{i, t}}$$
$$ b_{t+1} = b_{t} + \eta \cdot \frac{\partial E}{\partial b_{t}}$$</p>
<p>However, in the hidden layers, we don’t know what the desired output of the specific neurone is, this is where backpropagation comes in handy. Basically, we will propagate the errors of the output layer back through the network so that we can calculate the derivatives of the weights relative to the cost for all of the weights.</p>
<p>For this, we will need to define another equations. We replace the $\frac{\partial E}{\partial \hat{y}}$ with:</p>
<p>$$ \frac{\partial E}{\partial \hat{y}} = \sum w_i \cdot \frac{\partial E}{\partial z_i}$$</p>
<p>This tells us that for a node in a hidden layer, the derivative of the error relative to it&rsquo;s weights is equal to the weighted sums of the derivatives of the next layer. The weights used are simply the weights of the nodes in the next layer for that given node.</p>
<p>We can apply all the same equations as before when calculating the derivatives as before, we simply need to calculate the initial $\frac{\partial E}{\partial \hat{y}}$​ of the output layer, then use the derivative equations to propagate the derivatives through the network, then finally update the weights after we’ve calculated the relevant values.</p>
<h2 id="recap-of-the-algorithm">Recap of the algorithm<a hidden class="anchor" aria-hidden="true" href="#recap-of-the-algorithm">#</a></h2>
<p>Here is a recap of the algorithm to implement a simple Neural Network, assuming you have a collection of numerical input vectors and the desired true/false output label:</p>
<ol>
<li>Calculate the derivatives of the output neurons relative to the error using an input vector $x$ with a known output $y$</li>
</ol>
<p>$$ \frac{\partial E}{\partial w_i} = (y - \hat{y}) \cdot f&rsquo;(z) \cdot x_i$$
$$ \frac{\partial E}{\partial b} = (y - \hat{y}) \cdot f&rsquo;(z) $$</p>
<ol start="2">
<li>Propagate the errors of the last layer to the second last layer, then propagate the errors of the second last layer to the third last layer, etc. where $\hat{y}$​ is the output of a hidden node</li>
</ol>
<p>$$ \frac{\partial E}{\partial \hat{y}} = \sum w_i \cdot \frac{\partial E}{\partial z_i}$$</p>
<ol start="3">
<li>Once you know the derivatives of all the weights and biases for all the neurons, update them using the following rules:</li>
</ol>
<p>$$ w_{i,t+1} = w_{i, t} + \eta \cdot \frac{\partial E}{\partial w_{i, t}}$$
$$ b_{t+1} = b_{t} + \eta \cdot \frac{\partial E}{\partial b_{t}}$$</p>
<h2 id="python-implementation">Python Implementation<a hidden class="anchor" aria-hidden="true" href="#python-implementation">#</a></h2>
<p>Now that all the of the theoretical equations have been established, we can actually implement our model and test it on some real world data. For this example, we will be using the <a href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic">UCI ML Breast Cancer Wisconsin (Diagnostic) dataset</a>. You can download a copy of the dataset directly, or you can import it through the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html">Scikit learn dataset module</a>. We are trying to predict if a tumor is bening or malignant with several features such as the radius, symmetry, smoothness and texture.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_breast_cancer
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> math
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># implementation of sigmoid neurones in a layer</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SigmoidLayer</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># layer is defined by the input size and the number of neurones</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> __init__(self, input_size, number_of_neurones):
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>input_size <span style="color:#f92672">=</span> input_size
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>number_of_neurones <span style="color:#f92672">=</span> number_of_neurones
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># generate the weights as random numpy arrays of small values</span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># different initiliaztion schemes can be used for the weights with varying experimental success</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>biases <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.1</span>, number_of_neurones)
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>weights <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.1</span>, (number_of_neurones, input_size))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># defined the sigmoid function</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(self, z):
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">+</span> math<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>z))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># calculate the output of the neurones as a vector, based on an input vector to the layer</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">output</span>(self, x):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># initialize layer output, output will be of length self.number_of_neurones</span>
</span></span><span style="display:flex;"><span>		output_vector <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># iterate over each neurone in the layer</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> neurone <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>number_of_neurones):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># retrieve the bias term for the given neurone</span>
</span></span><span style="display:flex;"><span>			z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>biases[neurone]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># calculate the weighted sum and add it to the initial bias term</span>
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">for</span> feature <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>input_size):
</span></span><span style="display:flex;"><span>				z <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>weights[neurone][feature] <span style="color:#f92672">*</span> x[feature]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># apply the activation function</span>
</span></span><span style="display:flex;"><span>			neurone_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sigmoid(z)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># add the output of the given neurone to the layer output</span>
</span></span><span style="display:flex;"><span>			output_vector <span style="color:#f92672">+=</span> [neurone_output]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># return the output of the layer as a vector</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> numpy<span style="color:#f92672">.</span>array(output_vector)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># calculate the backpropagation step for the layer as well as update the weights, based on the last input, output</span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># and the derivatives of the layer, will output the derivatives needed for the previous layer</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backpropagate</span>(self, last_input, last_output, dE_dy, learning_rate):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		dE_dz <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># use the dE_dy of the layer to calculate the dE_dz of each neurone in the layer</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> neurone <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>number_of_neurones):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># apply the derivative of the sigmoid function</span>
</span></span><span style="display:flex;"><span>			neurone_dE_dz <span style="color:#f92672">=</span> last_output[neurone] <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> last_output[neurone]) <span style="color:#f92672">*</span> dE_dy[neurone]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># keep track of the derivatives of each neurone</span>
</span></span><span style="display:flex;"><span>			dE_dz <span style="color:#f92672">+=</span> [neurone_dE_dz]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		dE_dz <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array(dE_dz)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># use the dE_dz derivative as well as the last input to update the weights and biases</span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># this is the gradient descent step</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> neurone <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>number_of_neurones):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># update the bias of each neurone in the layer</span>
</span></span><span style="display:flex;"><span>			self<span style="color:#f92672">.</span>biases[neurone] <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> dE_dz[neurone]
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">for</span> feature <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>input_size):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>				<span style="color:#75715e"># calculate the derivative relative to each weight, then update each weight for each neurone</span>
</span></span><span style="display:flex;"><span>				self<span style="color:#f92672">.</span>weights[neurone][feature] <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> last_input[feature] <span style="color:#f92672">*</span> dE_dz[neurone]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># calculate the dE_dy derivative to be used by the following layer</span>
</span></span><span style="display:flex;"><span>		next_layer_dE_dy <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>zeros(self<span style="color:#f92672">.</span>input_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># iterate over each neurone</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> neurone <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>number_of_neurones):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># iterate over each weight</span>
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">for</span> feature <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>input_size):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>				<span style="color:#75715e"># calculate the derivative using the backpropagation rule</span>
</span></span><span style="display:flex;"><span>				next_layer_dE_dy[feature] <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>weights[neurone][feature] <span style="color:#f92672">*</span> dE_dz[neurone]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> next_layer_dE_dy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># implement Neural Network using sigmoid layer</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SigmoidFeedForwardNeuralNetwork</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># we need the number and sizes of the layers, the input size and the learning rate for the network</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> __init__(self, learning_rate, input_size, layer_sizes):
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>learning_rate <span style="color:#f92672">=</span> learning_rate
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># initialize each layer based on the defined sizes</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> range(len(layer_sizes)):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># input size of first layer is input size</span>
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">if</span> layer <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>				self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> [SigmoidLayer(input_size, layer_sizes[layer])]
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># input size of every other layer is the size of the previous layer</span>
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>				self<span style="color:#f92672">.</span>layers <span style="color:#f92672">+=</span> [SigmoidLayer(layer_sizes[layer <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>], layer_sizes[layer])]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># calculate the output of the neural network for a given input layer</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict_on_vector</span>(self, x):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># feed the output of each layer as the input to the next layer</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>			x <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>output(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># return the output of the last layer</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># calculate the outputs of the neural network for a list of vectors</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, X):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># calculate the prediction for each vector</span>
</span></span><span style="display:flex;"><span>		predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># make a prediction for each vector in the set</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X)):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			prediction <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>predict_on_vector(X[i])
</span></span><span style="display:flex;"><span>			predictions <span style="color:#f92672">+=</span> [prediction]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># return all of the predictions</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> numpy<span style="color:#f92672">.</span>array(predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(self, X, Y, iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># generate a list of indexes for the training samples</span>
</span></span><span style="display:flex;"><span>		training_samples <span style="color:#f92672">=</span> [i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(X))]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># do k iterations of training on the dataset</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(iterations):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># print training progress</span>
</span></span><span style="display:flex;"><span>			print(<span style="color:#e6db74">&#34;Epoch : &#34;</span> <span style="color:#f92672">+</span> str(k))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># randomly shuffle dataset to avoid getting stuck in local minima</span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># random.shuffle(training_samples)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># train on each sample</span>
</span></span><span style="display:flex;"><span>			<span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> training_samples:
</span></span><span style="display:flex;"><span>				self<span style="color:#f92672">.</span>train_on_vector(X[index], Y[index])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># train the Neural Network on a single vector, this training scheme is know as &#34;on-line&#34; training</span>
</span></span><span style="display:flex;"><span>	<span style="color:#75715e"># as the neural network is updated every time a new vector is given</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_on_vector</span>(self, x, y):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		outputs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>		inputs  <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># iterate over each layer and keep track of the inputs and output</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>			inputs <span style="color:#f92672">+=</span> [x]
</span></span><span style="display:flex;"><span>			x <span style="color:#f92672">=</span> layer<span style="color:#f92672">.</span>output(x)
</span></span><span style="display:flex;"><span>			outputs <span style="color:#f92672">+=</span> [x]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># calculate the error vector of the output neurones</span>
</span></span><span style="display:flex;"><span>		error_vector <span style="color:#f92672">=</span> x <span style="color:#f92672">-</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># iterate over each layer and apply the backpropagation, starting</span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(self<span style="color:#f92672">.</span>layers)):
</span></span><span style="display:flex;"><span>			index <span style="color:#f92672">=</span> len(self<span style="color:#f92672">.</span>layers) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> i
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			<span style="color:#75715e"># update the weights of the layers and retrieve the next layer&#39;s error vector</span>
</span></span><span style="display:flex;"><span>			error_vector <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[index]<span style="color:#f92672">.</span>backpropagate(inputs[index], outputs[index], error_vector, self<span style="color:#f92672">.</span>learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Apply the logistic regression model to the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset</span>
</span></span><span style="display:flex;"><span>X, Y <span style="color:#f92672">=</span> load_breast_cancer(return_X_y<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> [numpy<span style="color:#f92672">.</span>array([label]) <span style="color:#66d9ef">for</span> label <span style="color:#f92672">in</span> Y]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#split the data into training and testing sets</span>
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> X[:<span style="color:#ae81ff">100</span>]
</span></span><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> X[<span style="color:#ae81ff">100</span>:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Y_test <span style="color:#f92672">=</span> Y[:<span style="color:#ae81ff">100</span>]
</span></span><span style="display:flex;"><span>Y_train <span style="color:#f92672">=</span> Y[<span style="color:#ae81ff">100</span>:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize the model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SigmoidFeedForwardNeuralNetwork(<span style="color:#ae81ff">1e-1</span>, <span style="color:#ae81ff">2</span>, [<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train the model on the training set</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>train(X_train, Y_train, iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">25</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># calculate the accuracy on the training set</span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(predictions)):
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">if</span> predictions[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">and</span> Y_train[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>		accuracy <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">elif</span> predictions[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">and</span> Y_train[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>		accuracy <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>		
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Training Accuracy (%) = &#34;</span> <span style="color:#f92672">+</span> str(<span style="color:#ae81ff">100.</span> <span style="color:#f92672">*</span> accuracy <span style="color:#f92672">/</span> float(len(predictions))))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># calculate the accuracy on the testing set</span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accuracy <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(predictions)):
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">if</span> predictions[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">and</span> Y_test[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>		accuracy <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">elif</span> predictions[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">and</span> Y_test[i][<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>		accuracy <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Testing Accuracy (%) = &#34;</span> <span style="color:#f92672">+</span> str(<span style="color:#ae81ff">100.</span> <span style="color:#f92672">*</span> accuracy <span style="color:#f92672">/</span> float(len(predictions))))
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://philliams.github.io/tags/python/">Python</a></li>
      <li><a href="https://philliams.github.io/tags/machine-learning/">Machine Learning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://philliams.github.io/">An Hour A Day</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
