[{"content":"I wrote my last final this week, and so I wanted to take the opportunity to give a recap of my first two terms in Machine Learning at University College London.\nFirst weeks in London While courses started mid-October, I ended up moving to London sometime mid-September to give me some time to get settled in. Notably, I spent about 2 weeks reviewing “Mathematics for Machine Learning” by Deisenroth et al. Having worked in industry for the past 4 years, I was a bit rusty and so going through the practice problems was a great way to refresh my skills and get into the right mindset for the work to come.\nTerm 1 (Oct 2023 – Dec 2023) The first term consisted of 4 modules, with most of the content being foundational and expository. I found that the key takeaways of term 1 were all about the fundamentals and gaining fluency. Overall, we did a lot of math, and as a result built a lot of familiarity with the notation and concepts that show up often in machine learning. Some of the courses covered more theoretical aspects of Machine Learning, such as proving error bounds under various assumptions. Other courses were more procedural in nature, where we were expected to replicate and apply various algorithms for learning and inference.\nListed below are the modules I took in the first term. In terms of overall impression, I think that Bayesian Deep Learning and Graphical Models were quite excellent modules, while Supervised Learning and Robotics Systems Engineering were alright, but could a bit of polish on some rough edges.\nSupervised Learning This module was foundational, with a focus on theory and little application. We had 2 course works, which involved problems such as proving the \u0026ldquo;No Free Lunch\u0026rdquo; theorem or deriving error bounds for various learning algorithms. Also, the course had a final which involved solving some key problems presented throughout the module. Due to my engineering undergrad, I am very comfortable with calculus, but less so with more abstract branches of math. As such, I learned a lot in this course and acquired various mathematical tools. Throughout the course, the material is presented from first principles without stating the end-goal, such that everything is derived from known facts and assumptions. When unfamiliar with the content, this approach can feel very much like taking a path without knowing the destination, and so it is not clear which information is key or why a given identity is relevant. This course was quite interesting but assumes a lot in terms of pre-existing knowledge which can be difficult for students that lack a strong math background.\nGraphical Models This module covered a wide range of basic topics. From defining Belief Networks and Markov Networks, to performing inference in trees and loopy networks, we covered a wide range of topics. There was a single coursework, where we implemented many graph-based models and algorithms (such as solving the Ising model) in a wide variety of settings. This course was very expositional, in that we covered a wide range of topics in moderate depth and gained fluency in the language of graph-based models. Additionally, there was a final exam which was mainly focused around applying the algorithms learned to simple problems. This course was quite good, and I feel like I gained a lot. The lectures and evaluations felt fair, the lectures were clearly presented and laid out, and understanding was almost a given if you put in the effort to go through all the content in detail.\nBayesian Deep Learning This was by far my favorite course in the first term. The first half of the module covered Bayesian statistics, and how they can be applied to machine learning problems. In the second half of the term, we covered how introducing deep models complicates the setting, and techniques for tackling those issues (such as sampling, Bayesian last layer). There were two notebook-based course works, which tied in well with the content. The coursework involved implementing various models and algorithms and exploring their behaviour in simple settings. The final exam had more theory-based questions, such as explaining the limitations and settings in which one might use Bayesian methods and which algorithms to use. I believe that this module was a great mix of theory and applied learning. Additionally, given the number of papers which make use of Bayesian concepts, gaining a familiarity with Bayesian Deep Learning has greatly aided my comprehension of advanced research topics. This course is excellent, and I would highly recommend taking it.\nRobotics System Engineering This module was very engineering oriented. We covered the main concepts in robotics engineering, namely forward and inverse kinematics and dynamics. This course was 100% coursework based, split into three assignments. The first assignment required implementing forward kinematics using a ROS Noetic based simulator. While the concepts were not unduly difficult, the simulator\u0026rsquo;s complexity introduced a learning curve. The second assignment consisted of inverse kinematics, and the final coursework was related to forward and inverse dynamics. Overall, the course did a great job of imparting a working understanding of the problems and techniques used in robotics, though the simulator aspect could be improved.\nTerm 2 (Jan 2024 – April 2024) The second term was a mix of advanced and applied topics. Some modules, such as Machine Learning Seminar, built upon the first term and delved into more research-oriented topics. However, modules such as Applied Machine Learning were much more practical in nature. This mix was quite nice, as it allows students tailor the level of research or industry focus that they are interested in. Having worked in industry for a few years, I avoided the applied modules in favor of a heavily theory oriented second term, to expose myself to as many new topics as possible.\nListed below are the modules I took in term 2. I found Multi-Agent AI and ML Seminar to be excellent and among my favorite courses. Statistical NLP was a good course, but not particularly noteworthy. Finally, Reinforcement Learning was quite a painful course. Tunnel vision on theory while ignoring recent advances, poorly organized coursework and a difficult final detracted from what should be a very interesting and useful module.\nMachine Learning Seminar This course was by far the best module in term 2. We covered Gaussian Processes, Bayesian Optimization, Numerical Integration, Message Passing and Meta learning. The first 5 weeks were lectures on the listed topics, while the last 5 weeks involved reviewing various published papers and presenting them to the class. The evaluation for the course consisted of weekly quizzes, a paper review and a group presentation. The weekly quizzes were somewhat difficult, as a high level of detail is expected. The paper review required reading an assigned publication and acting as a conference reviewer. This gave insight into the peer-review process and dealing with published research. Finally, the group project was a paper review and subsequent presentation. The assigned papers were quite varied, and so listening to the different groups present gave a great overview of several topics. Overall, the course was excellent. The evaluations were fair and not overly difficult, and the content itself gave a great view into how theoretical topics appear in publications.\nStatistical Natural Language Processing This course covered a wide range of topics in Machine Learning, starting from topics such as n-gram models and feature engineering, all the way to Large Language Models, safety and adversarial attacks. The evaluation was a single open-ended research project on an NLP related topic. Since I had done research before, it was relatively easy to achieve a good mark. However, several students who had never dealt with either NLP or research before found it difficult. Overall, the course was fine, with a bit of a sink-or-swim dynamic due to the nature of the evaluation.\nReinforcement Learning This course was very theoretical. The bulk of the module related to topics such as the Bellman Equations, the fundamentals of dynamic programming, and showing that algorithms do or do not converge to optimality under various convergence. The evaluation consisted of a notebook-based coursework and a final exam. The notebook itself was a mix of practical questions, such as implementing Policy Gradients or Upper-Confidence Bound, as well as theoretical questions, such as proving that a given operator converges to optimality or deriving the expected weight update under a Markov-Decision Process. The final consisted of questions such as deriving updates given a Markov Decision Process or recalling definitions and proofs of several algorithms covered throughout the term. Similarly to Supervised Learning in term 1, this module often presented information without providing context of motivation. Many students seemed disappointed with the module, as it did not cover the implementation of Reinforcement Learning in practical settings, but rather was entirely focused on the theoretical underpinnings of various algorithms. Overall, this course was the worst module across both terms, as the course itself was quite difficult and poorly structured.\nMulti-Agent Artificial Intelligence This course was quite excellent and did a great job of covering theoretical topics and relating them to practical settings. The module covered topics such as game theory, Nash equilibria and multi-agent Reinforcement Learning. The evaluations were a notebook-based coursework and an open-ended research project. The notebook coursework did a great job of bridging the theoretical concepts covered in class, and the practical implementations. Some of the questions involved implementing Support Enumeration, computing optimal policies for given matrix games, and solving potential games. Similarly to Statistical NLP, the open-ended research project can be quite easy or difficult depending on the amount of research a student has done prior. Overall, the course was excellent and did a pretty good job of complementing the more theoretical RL course.\nConclusion Going into this Master\u0026rsquo;s, I was expecting that the content would be a 50/50 split between math and computer science. Instead, I would say that the content was 99.9% math and theory oriented. However, I\u0026rsquo;ve quite enjoyed this, I feel like I\u0026rsquo;ve learned way more than I expected, and I\u0026rsquo;ve come out on the other side with a much deeper grasp of Machine Learning. Despite some rough edges, most of the courses were quite competently run and covered a range of interesting and relevant topics. Finally, the master\u0026rsquo;s is challenging in terms of the difficulty and volume of information and the amount of coursework we needed to complete. Now that I’ve finished finals, I\u0026rsquo;ll be working on my thesis over the summer, and I\u0026rsquo;ll be hoping to put everything I learned into practice.\n","permalink":"https://philliams.github.io/posts/ucl/ucl_part_1/","summary":"I wrote my last final this week, and so I wanted to take the opportunity to give a recap of my first two terms in Machine Learning at University College London.\nFirst weeks in London While courses started mid-October, I ended up moving to London sometime mid-September to give me some time to get settled in. Notably, I spent about 2 weeks reviewing “Mathematics for Machine Learning” by Deisenroth et al.","title":"University College London - Part 1"},{"content":"This September I am taking a leave from my job at $dayjob$ to pursue a Master\u0026rsquo;s of Science in Machine Learning at University College London. As I am opening a new chapter in my life, I want to reflect and write down some of the key themes from my first 5 years in industry.\nBackground Context At $dayjob$, the bulk of my work was focused on time series forecasting for supply chain management. In plain english, we built systems to forecast how much of a given item will sell at a given store on a given day. Producing these forecasts involved a wide variety of technologies from infrastructure, to big data, and finally various machine learning models. I was fortunate enough to join the Machine Learning team as it was getting started, and was able to work on the project from a green-field initiative to a large-scale product used by customers. In that time, I went from intern, to Machine Learning Developer, and finally Machine Learning Team Lead:\nMachine Learning Intern (summer 2018) Worked on incorporating new sources of data into our time series forecasts Technologies: Python, Jupyter notebooks, Scikit-Learn, git Machine Learning Developer (May 2019 - June 2022) Worked on implemented ML and big data systems for large-scale time series forecasting Technologies: Docker, Kubernetes, Python, PySpark, Dask, Argo Workflows, Github Actions, Scikit-learn, Optuna Machine Learning Team Lead (June 2022 - August 2023) Lead feature generation team (~8 developers) and was responsible for Feature Generation module and related functionality During my time as team lead, we overcame several challenges and had some big wins. Notably, doing a full re-write of the module to improve runtime by 5x, and implementing incremental logic to improve forecasting time by a factor of 20x. As such, this post will focus on things that I found key for succesfully running a large project rather than specific technical addvice.\nKey Concepts and Takeaways When going from a green-field project to an enterprise product used by large-scale customers, there were 4 key ingredients to success. These four elements do not encapsulate everything needed to run a project, but if any one of these elements were missing, then the project would have been in a much rougher spot.\nOwnership Meta-strategies Principled Development Technical Excellence Ownership As a team lead, I found that ownership was probably my single most imporant duty. Making sure that the right decisions were chosen, and ensuring that the that things that need to be done, get done was the difference between a great and terrible outcome. Sometimes, this mean doing technical due diligence to make sure that our designs made sense, that things were easy to test and deploy, and overall ensure the quality of the code. Other times, ownership meant involving myself into discussions outside of my direct responsibilities to avoid problems down the road.\nOwnership is also about making sure that the things that need to be done, get done\nPrincipled Development In chess, there is the concept of \u0026ldquo;Principled Play\u0026rdquo;. The idea is that there is a set of principles that you should strive to follow, and that you should only break them if you have a very strong reason to deviate from the principles. In the context of software development, this means having a consistent approach to design and problem solving. During my time at $dayjob$, I developped a framework of unambiguous, actionable and (mostly) objective heuristics for evaluating designs and architecture. I want to emphasize that the heuristics need to be actionable, unambiguous and objective. A lot of the time, the advice that is given is that you need to write \u0026ldquo;clean\u0026rdquo; code, or \u0026ldquo;good\u0026rdquo; code. That advice is worse than useless:\nSubjective : \u0026ldquo;good\u0026rdquo;/\u0026ldquo;clean\u0026rdquo; are really just matters of taste that will vary a lot between developers, and will vary heavily based on experience, language, framework and team culture. Not Actionable : There is no specific recommendation of how to make the code \u0026ldquo;good\u0026rdquo; or \u0026ldquo;clean\u0026rdquo;. But rather the implication is that there exists some nebulous definition of \u0026ldquo;good\u0026rdquo; and that you should just know how to achieve it. Ambiguous : it is not clear how to quantify or measure if something is \u0026ldquo;good\u0026rdquo; or \u0026ldquo;bad\u0026rdquo; at any level. You can\u0026rsquo;t point to a specific part of the code and give a clear recommendation on how to improve it. However, there are some heuristics that are unambiguous, actional and objective. These allow you to align with other team members for a consistent development philophy and style, make the implementation of the code more cohesive and allows you to provide clear and actionable feedback on design and pull requests. Some of my favorite ones are:\nTestable: Is your code easy to test? The ease of testing can be measure by how much mocking you need to do, the complexity (less complex is better) intermediate state you need to set up before you can test, and how much of the code can be tested without needing to setup a specific environment or deploy it (fewer requriements is better). Deployable: Is your code easy to deploy? The ease of deployment can be measure by how much configuration/parameters are needed to get something simple up and running, how many other services need to be deployed as prerequisites, how long it takes to deploy (faster is better), and how much of the deployment can be feasibly automated (more automation is better). Debuggability : Is your code easy to debug? The ease of debugging can be measure by how much effort is needed to access some internal state (easier to access is better), if the application can be run with a debugger enabled, and how long the iteration cycle to make and test a change is (shorter iteration cycle is better). In my experience, having a set of unambiguous, actionable and objective guiding principled for development has been key to aligning on design, and has led to robust, performant and high-quality software. A full write-up of my thoughts can be found in the Principled Development post.\nMeta Strategies A meta-strategy is a strategy for choosing which strategy to apply in a given situation. For example, monoliths an micro-services are both software development strategies that have various advantages and disadvantages. Rather than always using either monoliths or micro-services, a meta-strategy would give you a consistent way to choose which architecture, design or strategy to use in any given situation.\nMeta-strategies can also apply to more team or project oriented questions, and can be used in a technical or non-technical capacity. For example, decisions such as whether to refactor code or keep it as-is, whether to pick design A or B, or whether to go with business plan X or Y, can all be resolved if you have a robust meta-strategy.\nMy personal meta-strategy is outlined in the Meta-Strategy post, but can be roughly boiled down to a few key steps:\nBe clear and unambiguous in specifying exactly the problem you are trying to solve Identify the win conditions that need to be met (requirements) and loss conditions that need to be avoided (i.e. missing deadlines) Evaluate strategies based on how they meet the requirements and avoid the loss conditions. Avoid any complexity beyond exactly the requirements. Having a clear and actionable meta-strategy allows for difficult decisions to be made in a consistent and justifiable way, and can even help communicate the reasoning to various stake holders.\nTechnical Excellence Technical excellence is related to both meta-strategies and principled development. Having a strong understandings of the limits and features of various technologies, architectures and algorithms provides both more information to make better decisions and the capcity for better execution of the chosen strategy. Technical excellence feeds into the principles you adopt and informs the variables accounted for in your meta-strategy. On the other hand, robust principles and meta-strategy give context and ground your technical expertise.\nWe are what we repeatedly do. Excellence, then, is not an act but a habit. - Aristotle\nIn particular, technical excellence is not an innate characteristic, but rather something that needs to be diligently practiced at both an individual level and at a team level. At an individual level, technical excellence is achieved by learning and experimenting. Trying new technologies, running experiments for various architectures, and exploring new best practices. As a team, maintaining code quality is like fighting against gravity. Tt is difficult to for the code quality to stay at the same level, it is easy to make the code worse, and almost impossible to improve code quality. However, improving code quality is still possible even on large enterprise projects. The key is to have the discipline to continuously improve developper experience. Reducing build times, enabled debuggers, separating concerns so tests can be executed locally, and pushing back on complexity will ensure that iteration times are short and that developers are nimble and productive. From there, it is much easier to make changes as needed. From there, it is a lot easier to achieve sustainable technical excellence, since the team has all the tools to modify large portions of code quickly and with little risk.\nFurther Reading This post was intended as a quick overview of the main four pillars that led to the ongoing success of my work and team at $dayjob$. More detailed write-ups that dig into the nuances are available:\nOwnership Meta-Strategy Principled Development and Technical Excellence ","permalink":"https://philliams.github.io/posts/career_5_years/recap/","summary":"This September I am taking a leave from my job at $dayjob$ to pursue a Master\u0026rsquo;s of Science in Machine Learning at University College London. As I am opening a new chapter in my life, I want to reflect and write down some of the key themes from my first 5 years in industry.\nBackground Context At $dayjob$, the bulk of my work was focused on time series forecasting for supply chain management.","title":"University College London - Part 0"},{"content":"A lot of blog posts discuss the correct architecture or strategy for a given situation. In this post, we will not discuss strategies or architectures, but rather a meta-strategy. A meta-strategy is a strategy for choosing strategies. In the context of software development, a meta-strategy would not be a recommendation of a particular architecture or tech stack, but rather actionable advice on how to evaluate and choose architectures and implenentations. In this post, I will outline 6 core concepts and how they fit together to form my own personal meta-strategy:\nWin conditions Lose conditions Tempo Scaling Variance Risk Win and Lose conditions When planning software projects, win and lose conditions are key. The basic idea is that:\nWin conditions are conditions, that if met, means the project has succeeded\nLose conditions are conditions, that if met, mean the project has failed\nAn example of a win conditions might: be delivering a certain feature by a certain date, meeting all the requirements, or improving performance by X%.\nAn example of a lose conditions might be: failing to deliver a certain feature by the deadline, failing to meet Service Level Agreements (SLAs), or missing some requirements.\nWhen making decisions about designs, architecture, tech stacks, or development plans, you need to make sure that the option chosen will satisfy your win conditions while avoiding your lose conditions. If you only have 100mb of data, choosing a complex big-data tool is the wrong approach, because it increases your chances of hitting a lose condition (missing deadlines) without contributing to your win conditions.\nAdditionally, it is important to note that none of the example win or lose conditions mention anything about code quality. Code quality is a tool for meeting win conditions and avoiding lose conditions. Poor code quality can mean that the software is not ready to be released on time, while good code quality may mean that new features are added on time and under budget. On the other hand, extremely high quality code delivered in 5 years does not help you if you need to meet a deadline in 6 months. Code quality, performance, scalability, tech stacks and architectures need to be chosen with a clear purpose in line. In the next section, I will discuss the concepts of tempo and scaling, and how they can help make technical decisions.\nTempo and Scaling Tempo and scaling are two concepts borrowed from competitive video games that are extremely relevant to software development. Tempo and scaling are two independent attributes that are given to a given move or choice:\nA tempo move is a move that gives immediate value, and may be neutral or harmful in the long term\nA scaling move is a move that gives increasing value over time, but may be neutral or harmful in the short term\nAn example of a tempo move is to implement a quick, hacky fix that solves a bug today, but will introduce technical debt that will need to be cleaned up later. An example of a scaling move is taking a few weeks to refactor some code to improve maintainability over time. It is worth noting a well, that tempo and scaling are not mutually exclusive. A very good design may provide both tempo and scaling. Inversely, a bad design may provide neither tempo or scaling while taking up time and effort.\nThe concept of tempo and scaling is tightly coupled to the concept of win/lose conditions. Tempo and scaling can only be meaningfully discussed through the lens of specific objectives in time. In fact, you can imagine win conditions and lose conditions as set points spaced out in time. You need to balance scaling and tempo so that you are able to meet your short term obligations, while not putting yourself in a position where you are not able to meet your long-term obligations. For example, taking on technical today to meet an objective next week may make sense. On the other hand, taking on technical debt now to get started quickly may not make sense if the deadline is in a year.\nVariance and Risk The last two key concepts for meta-strategies are variance and risk:\nVariance describes how wide the range of plausible outcomes are. A strategy that is guaranteed to produce a given result is low-variance. A strategy that could produce many different outcomes is high-variance.\nRisk describes how likely a project is to fail. A project almost guaranteed to succeed is low-risk, while a project guaranteed to fail is high-risk.\nAdditionally, there are three main situations that a project can find itself in:\nDefault Win : a project that has a very high chance of succeeding for a given strategy Uncertain : a project that has an equal chance of failure and success for a given strategy Default Lose : a project that has a very high chance of failing for a given strategy When evaluating strategies, we often lack the language and framework for properly discussing risks. Often, when people say that something is risky, they are really saying that the outcome is uncertain (high-variance). However, high-variance is not always a bad thing. For example, if your project is in a default-lose state, it may be worth adopting a high-variance strategy to get you into the uncertain state. This may seem unintuitive, but if you have a 95% chance of failure for a low-variance strategy, or a 50/50 chance of success or failure, then it is worth taking a gamble. You still have a 50% chance of losing, but your chances of success increased by 10x from 5% to 50%. Through this lens, we can reason about various strategies in a more analytic way, and can discover unintuitive strategies that are very good when explored thorougly.\nOne-way doors An interesting concept is the \u0026ldquo;One-Way Door\u0026rdquo;. A one-way door is a decision that once made, is impossible or very expensive to undo. On the other hand, a two-way door is a decision that can easily be reverted down the line. One-way doors should be deferred to the last possible moment. If you implement a one-way door too early, you take on complexity now and open yourself up to very painful technical issues if the requirements were to change in the meantime. Additionally, if you are forced to implement a one-way door, then you likely have a very compelling reason to do so.\nIn other words, two-way doors decision can be made more lightly and eagerly, while one-way door decisions should be deferred to the last possible moment and should be considered much more carefully.\nMy Meta-Strategy Now that we have all the concepts for a meta-strategy, we can discuss my particular meta-strategy:\nLay out all the win conditions and lose conditions before any planning or design is done Collect a wide variety of different solutions Evaluate which solution most closely matches the win conditions missing win conditions or hitting lose conditions is bad wasted effort or extra complexity beyond win conditions is bad if the strategy is default-win, high-variance is bad, low-variance is good -\u0026gt; risk is low, don\u0026rsquo;t gamble if the strategy is default-lose, low-variance is bad, high-variance is good -\u0026gt; risk is high, gamble for chance of success Once a tentative solution is identified break down solution into implementation effort identify intermediate milestones that need to be met identify the minimum amount of tempo needed to achieve intermediate and final milestones any extra tempo beyond the minimum needed amount is bad any un-allocated capacity is allocated to scaling more un-allocated capacity is good, as it allows for more scaling effort if solution turns out to not be suitable, revisit other solutions Once a final solution is chosen break down implementation work needed plan immediate implementation needed for short-term deliverables plan scaling work for any unused capacity clean up technical debt improve performance improve tooling improve testing etc. if possible, implement scaling effort early to enable later tempo work After implementation is done, identify pain points to be resolved via future scaling work. Case Study To put the meta-strategy into context, let\u0026rsquo;s take a look at a case study that reflects a common situation that many projects find themselves in:\nContext Company A is implementing product Z. However, the project is way behind schedule and is currently not meeting the requirements. We need to pick a strategy to salvage the project. We are dealing with a legacy codebase that is unreliable, performs poorly and is poorly tested.\nThe current state completes a simple Machine Learning calculation in 3 days and has had several severe outages.\nWin conditions To achieve success, we need to implement functionality to perform an improved Machine Learning calculation in under 12 hours on a large volume of data. The deadline is in 3 months.\nLose conditions To avoid failure, the project needs to be free of any major bugs, needs to be deployable by the professional services team, and must be able to handle real-world loads without failing.\nStrategies Strategy 1 : This strategy is to follow the current development plan and to incrementally remedy the situation\nVariance : Low Risk : Very High Tempo : Low Scaling : Low Prognosis : 95% failure Strategy 2 : This strategy is to completely throw the old code away and re-write from scratch with new technologies\nVariance : Very high Risk : High Tempo : Moderate Scaling : Very High Prognosis : 50% failure Strategy 3 : This strategy is to add some testing and maintain the outer API, but to refactor the internals chunk by chunk\nVariance : Moderate Risk : Moderate Tempo : High Scaling : Moderate-High Prognosis : 25% failure Strategy Chosen In this case, we want to choose strategy #3. Strategy #1 is not suitable, since we are almost guaranteed to fail. Strategy #2 is an improvement over #1, since we have a much higher chance of success. However, with strategy #2 we are taking unneccesary risk. #3 is thus ideal, since we have a high chance of success without taking unneccesary risks.\nImplementation Breakdown We have 3 large \u0026ldquo;chunks\u0026rdquo; of internal logic to replace. Each chunk will take ~3 weeks to update. Additionally, we need a month to re-write all of tests. However, if we dedicate 2 weeks at the start of the development to set up the tooling, we can cut the work down to ~2 weeks per chunk and ~3 weeks for the tests. The timeline would then look like:\nweeks 0-2 : set up tooling weeks 2-5 : add new tests weeks 5-7 : refactor chunk #1 weeks 7-9 : refactor chunk #2 weeks 9-11 : refactor chunk #3 week 12: finalize work for deadline. Thus, we have gone from a project that was way behind and most likely to fail, to a project that delivers the requirements (i.e. performance boost and improved ML logic) on time while improving the overall code quality. We met the win conditions, avoided the lose conditions, avoided unneccesary effort and were able to allocate some effort to improving developer experience.\n","permalink":"https://philliams.github.io/posts/career_5_years/meta_strategy/","summary":"A lot of blog posts discuss the correct architecture or strategy for a given situation. In this post, we will not discuss strategies or architectures, but rather a meta-strategy. A meta-strategy is a strategy for choosing strategies. In the context of software development, a meta-strategy would not be a recommendation of a particular architecture or tech stack, but rather actionable advice on how to evaluate and choose architectures and implenentations. In this post, I will outline 6 core concepts and how they fit together to form my own personal meta-strategy:","title":"Meta-strategies for Software Development"},{"content":"From low-level design to high-level project planning, ownership is central to succesfully enacting change. In this post, I will talk about what ownership is, and several ways to achieve effective ownership.\nWhat is ownership? Before we can talk about what ownership means and how to achieve it, we need to agree on a definition of ownership. For me, ownership has three pillars: the authority to influence outcomes, responsibility for the outcomes, and a commitment to making sure that the things that need to be done, get done.\nOwnership is a combination of authority to drive the course of action taken, and responsibility for the outcome.\nWhen it comes to a software project, ownership can manifest itself in a variety of ways and in multiple scopes. At a low-level, ownership may mean pushing back on poor implementation in pull requests. At a mid-level, ownership may mean doing technical due diligence to ensure high quality designs. At a high-level, ownership means taking a longer view on things; making sure that the overall direction of the product is correct, coordinating with other teams to avoid issues, and fostering growth within the team.\nAchieving ownership Ownership and authority typically come in two flavors : \u0026ldquo;hard\u0026rdquo; power and \u0026ldquo;soft\u0026rdquo; power. Hard power comes in the form of your superior assigning you tasks. On the other hand, soft power is being able to achieve ends by persuading peers or superiors, without a way to force them to adopt a particular course of action. Soft power is more broadly applicable and is more interesting, since it can be achieved regardless of seniority and can be used outside of your immediate team.\nThere are a couple of ways to achieve ownership via soft power. First of all, you can achive ownership by being a known expert for a given area. If you are the most knowledgeable person about a particular area (but not neccesarily the most senior overall), then people will consult you for future developments in that area. Consequently, you will be able to achieve ownership, as you will both be able to influence decision making via your input, as well as having responsibility by virtue of being the known \u0026ldquo;expert\u0026rdquo; for that area.\nAnother way of achieve ownership and soft power is to be involved in ongoing discussions and mentorship. It can be difficult to suddenly convince someone of an idea the first time they encounter it. But on the flipside, if you\u0026rsquo;ve been having discussions about similar topics on an ongoing basis, and align yourselves before the fact, it can be much easier to get buy in, since the ground work has already been laid.\nFinally, a useful way of gaining ownership is by being a champion. If you have an idea, and you pitch it to the team, and you don\u0026rsquo;t hear a no, then go ahead and implement it! This way, you can gain ownership by virtue of taking initiative and being the first person to implement a given piece of functionality. Being a champion can be an incredibly effective way of gaining ownership and cutting through red tape. That being said, being a champion and having ownership means being comfortable with the risk of being wrong in a very public way.\nConclusions In conclusion, ownership is key to enacting change and can be achieved via a couple of paths. But the core takeaway is that ownership is about making sure that the right decisions are made, and the the work that needs to get done, gets done. This may mean joining meetings to raise concerns, reaching out to other teams to coordinate efforts and align technical vision, or something as simple as taking initiative and implementing a new tool in a code base.\n","permalink":"https://philliams.github.io/posts/career_5_years/ownership/","summary":"From low-level design to high-level project planning, ownership is central to succesfully enacting change. In this post, I will talk about what ownership is, and several ways to achieve effective ownership.\nWhat is ownership? Before we can talk about what ownership means and how to achieve it, we need to agree on a definition of ownership. For me, ownership has three pillars: the authority to influence outcomes, responsibility for the outcomes, and a commitment to making sure that the things that need to be done, get done.","title":"Effective Ownership in Software Development"},{"content":"I am a very strong believer in consistency. In fact, I think that consistency is at the heart of any engineering endeavor. Towards this end, I implemented what I call \u0026ldquo;Principled Development\u0026rdquo; into my daily work as a Machine Learning developer.\nIn this post, \u0026ldquo;Principled Development\u0026rdquo; refects to having a framework of clear and actionable heuristics for making technical decisions, and evaluating designs and implementations. Having a framework for making consistent technical decisions is incredibly valuable, it allows for alignment and a unified technical vision within the team, provides a scaffold for ideologically consistent development over time, and increases overall code quality.\nThe framework in question should be a set of clear, unambiguous, actionable and objective principles and heuristics to guide development. For example, \u0026ldquo;good code\u0026rdquo;, \u0026ldquo;clean code\u0026rdquo; or \u0026ldquo;better abstractions\u0026rdquo; are all completely useless when evaluating code. For example, \u0026ldquo;good code\u0026rdquo; is a matter of personal taste, and does not relate to any actionable insighs to increasing code quality. On the other hand \u0026ldquo;easy to test\u0026rdquo;, \u0026ldquo;easy to deploy\u0026rdquo; and \u0026ldquo;easy to modify\u0026rdquo; are all clear, unambiguous and actionable principles that lead to better code quality and better developer experience. Code that requires a lot of mocking or complex internal state is hard to test, and should be refactored to be more easily tested. Code that requires complex and slow deployment makes development more difficult, and should be changed to be deployed quickly via a single command. Code that requires a lot of pain staking changes to configuration files is hard to modify, and should be refactored such that changes can be quickly tested with minimal overhead.\nThe specific choice of principles may be different from one developer to another, or from one team to another. This is fine and expected, the key take-away is that developers should strive to develop their own set of principles, and to make every decision and to validate every design against their set of principles. Additionally, the principles would be expected to evolve over time and in the presence of new information.\nMy Principles My personal framework for principled development has 8 core tenets:\nExplicitness Single source of truth Ease of testing Short iteration cycles Fail eagerly Fail explicitely Thin and flat Ease of debugging Explicitness When reading code for a given function/class/module/etc., all definitions, dependencies and inputs should be explicitely identifiable from within the code. More precisely, a developer should be able to navigate the code where a function/data is used, to where the function/data is defined without any prior knowledge of the codebase.\nLet\u0026rsquo;s look at an example of a simple function that processes a dictionary that is expected to have a given schema. This data is static metadata that changes rarely, and is checked into the version control system. In the first example, we will load the dictionary from a JSON file stored on disk.\nimport os def main(): filepath = os.environ[\u0026#39;SOME_FILE_PATH\u0026#39;] data = load_json(filepath) do_logic(data) if __name__ == \u0026#39;__main__\u0026#39;: main() The example above is extremely hard to validate without access to a running copy of the application. Some uncertainties include: how/where was the environment variable set? What is the value of the environment variable? What file was loaded? What are the contents/expected schema for the data?\nAll of these questions can be answered, but require either a running application, or prior knowledge of the codebase, since there are no imports, type hints or definitions that can be used to back-track.\nFor an alternative implementation, we can instead put the data into a separate module, define some type hints, and import the module itself.\n# in data_module.py from typing import TypedDict class DummySchema(TypedDict): foo: str bar: int data: DummySchema = {\u0026#39;foo\u0026#39;: \u0026#39;bar\u0026#39;, \u0026#39;bar\u0026#39; : 1234} def get_data() -\u0026gt; DummySchema: return data # in the main_script.py import data_module def main(): data = data_module.get_data() do_logic(data) if __name__ == \u0026#39;__main__\u0026#39;: main() The example above achieves the same functionality, as the first example, but in a much more explicit way. From the main() function, I can immediately view the schema via the type hint DummySchema, as well as validate the contents in data_module.data.\nSingle source of truth For every process, and within the code, there should be a single, unambiguous source of truth. This means that we should only have a single approved/supported way of running tests, linting, deploying the application, etc. that is used both locally by developers and as a part of the build pipeline.\nFor example, in my sample repo Python-Prefab, a handful of commands are specified in the Makefile and are used both locally and within the CI pipeline.\nEase of testing Testing code is necessary and gives increased confidence when making significant changes. Code should strive to be as easy to test as possible. Tricky mocking logic, difficulties isolating global/environment variables or strict coupling to deployed environments should be eliminated. An ideal state to strive for, is to be able to test the majority of the logic (\u0026gt;90%) entire within unit tests with no mocking.\nAs an example, let\u0026rsquo;s look at two implementations of some logic that will load data from the database, perform some transformations, and persist the result:\nimport psycopg2 # Library for making db connections def do_main(): # create query string for load query_string_load = ... # load data from db connection = psycopg2.connect(...) cursor = connection.cursor() cursor.execute(query_string_load) data = cursor.fetchall() # apply transformations for record in data: # do some data transformations do_mutate_data(record) # create query string for save query_string_save = ... cursor.execute(query_string_save, data) if __name__ == \u0026#34;__main__\u0026#34;: do_main() In the example above, you would need to mock creating the db connection, executing queries and saving the data. This means that : 1) you need to mock a significant amount of the database interface (which will be a lot of work) and 2) your tests are tightly coupled to the internals of your logic.\nAn alternative implementation may look something like:\n# in main_script.py def load(cursor): # create query string for load query_string_load = ... # load data from db cursor.execute(query_string_load) data = cursor.fetchall() return data def save(cursor, data): # create query string for save query_string_save = ... cursor.execute(query_string_save, data) def do_logic(data): # apply transformations for record in data: # do some data transformations do_mutate_data(record) return data def do_main(): connection = psycopg2.connect(...) cursor = connection.cursor() data = load(cursor) result = do_logic(data) save(cursor, result) if __name__ == \u0026#34;__main__\u0026#34;: do_main() In the second example above, we can now test the do_logic function on it\u0026rsquo;s own, by simply passing in the correct data structure. This allows my tests to be decoupled from the IO logic. Additionally, if I want to test do_main(), I simple need to mock load(...) to return a premade object and save(...) can be mocked with a simple function to record the result in memory to be validated. Consequently, my tests are : 1) less effort to write, 2) decoupled from the internals of both my logic and the library api, and 3) allow me to test my entire main script within a unit test (IO excepted).\nShort iteration cycles There should be an emphasis on short iteration cycles. If significant logic can be run or tested locally, the iteration time for testing a change may be on the order of seconds. On the other hand, the same code may take many minutes or hours to deploy and test in a staging environment. Short iteration loops allow for greater productivity, code quality, and reduces risk when making changes. Consequently, architectures that enable short iteration cycles should be heavily encouraged when choosing designs.\nSome tricks for enabling short iteration cycles include:\nLocal only development Moving detailed checks from integration tests to unit tests Using static analysis tools Fail eagerly Code should check for reasonably expected edge cases that would cause the code to fail. Additionally, the code should fail at the earliest possible opportunity with descriptive and actionable error messages that allow for the code to be debugged with needing to manually instrument the code and re-run the application. In essence, if any inputs are wrong, malformed, corrupted, incorrectly configured, etc., an exception should be raised that will allow for a developer to quickly identify the source of the issue.\nAn extreme version of this is \u0026ldquo;Crash-only Software\u0026rdquo;, where any unexpected failure causes the system to outright crash. In the short term, this makes the system more flakey and prone to crashing. But in the long term, the system becomes more robust as any issues are immediatly made visible and resolved.\nFail explicitely In Machine Learning applications, it is key not only to have the application run correctly, but also to ensure that the model and data are accurate. Towards this end, silent failures are a significant source of issues. Silent failures can degrade data quality, degrade model quality, or cause issues in training and inference in a hard-to-detect manner.\nAs such, various mechanicsm that swallow exceptions such be avoided. For example, blanket catch statements should not be used. Rather only specific and expected exception should be caught. If a library or dependency is expected to sometimes raise exceptions, those should be caught and handled as part of the regular control flow. If an exception is not expected, then it should be raised, logged, or otherwise surfaced. Silent failures are some of the hardest errors to fix and should be avoided.\nThin and flat Code should be kept as flat and thin as possible. Minimize nested logic, abstraction, and inheritance. There are many reasons for this: 1) deeper code has been showed to be disproportionally hard to understand (Maheswaran, K., and A. Aloysius. \u0026ldquo;Cognitive weighted inherited class complexity metric.\u0026rdquo; Procedia Computer Science 125 (2018): 297-304.) and 2) deeper code makes debugging much harder, as the internal state is less accessible and requires significant changes to examine the behaviour at a low level.\nEase of debugging Finally, the code should be as easy to debug as possible. In an abstract sense, this means making the internal state of the code as accessible as possible. In practice, this means a few things: 1) minimizing the number of abstraction layers between the developer and the running code, 2) enabling tools such as debuggers and log aggregation services, and 3) setting up sample datasets and environments to quickly set up and test the code in a variety of situations.\nConclusion In conclusion, meta-strategies are a powerful tool for making and communicating technical decisions. Additionally, meta-strategies should be clear and actionable, with some examples and inspiration given by my personal meta-strategy. Finally, meta-strategies can be shared within a broader team or organization, as a way of aligning developers and building more consistent and hig-quality software.\n","permalink":"https://philliams.github.io/posts/career_5_years/principled_excellence/","summary":"I am a very strong believer in consistency. In fact, I think that consistency is at the heart of any engineering endeavor. Towards this end, I implemented what I call \u0026ldquo;Principled Development\u0026rdquo; into my daily work as a Machine Learning developer.\nIn this post, \u0026ldquo;Principled Development\u0026rdquo; refects to having a framework of clear and actionable heuristics for making technical decisions, and evaluating designs and implementations. Having a framework for making consistent technical decisions is incredibly valuable, it allows for alignment and a unified technical vision within the team, provides a scaffold for ideologically consistent development over time, and increases overall code quality.","title":"Principled Development"},{"content":"Generators and Coroutines are very powerful tools in Python that can help simplify logic, speed up data-intensive programs or provide flexible and re-useable APIs. In this post, we will explore three main concepts in Python : Generators, Coroutines and Cogenerators.\nGenerators Generators in Python are objects that contain some sort of internal state, and know how to produce the \u0026ldquo;next\u0026rdquo; value in a sequence.\nBefore we talk about what generators are, we should talk about what problems they can help solve! By using generators you can:\nIterating over data structures in a way that decouples your logic from the data structure Generators can be used to replace callbacks with iteration, you can perform work, and yield a value whenever you want to report back to the caller Processing data in small chunks so that only a small portion of the data is ever loaded into memory (Lazy evaluation) Generators provide many methods, but the ones that we will focus on are __iter__ and __next__. __next__ allows you to call value = next(some_generator); this call will tell the generator to update it\u0026rsquo;s internal state and give you the next value. __iter__ allows your generator to implement the Iterator interface such that you can iterate over your generator using the element for element in some_generator syntax (usually, if you already implement __next__ your __iter__ will just return self, otherwise you can have an object create a new iterable object and return that).\nGenerators using classes Let\u0026rsquo;s define a simple program that will use a generator to produce Fibonacci numbers. In this example, we will implement the generator class ourselves.\nclass FibonacciGenerator: def __init__(self, n1=0, n2=1, max_iters=100): self.max_iters = max_iters self.current_iter = 0 self.n1 = n1 self.n2 = n2 def __next__(self): if self.current_iter \u0026lt; self.max_iters: self.current_iter += 1 sum_ = self.n1 + self.n2 self.n1 = self.n2 self.n2 = sum_ return sum_ else: raise StopIteration def __iter__(self): return self In the __init__, we set the current number of iteration, the max number of iterations and the first two Fibonacci numbers n1 and n2. In the __next__ method, we check if we are under the maximum number of iterations and if so compute the next Fibonacci number, update n1 and n2 and then return the next Fibonacci number. __iter__ is very simple and we can just return the object since the FibonacciGenerator class implements __next__.\nWe can then use this class to easily compute and iterate over Fibonacci numbers. We can exhaust all of the numbers by invoking [e for e in gen]. If we try to get another value after the generator has been used up, an exception will be raised.\ngen = FibonacciGenerator(max_iters=10) nums = [e for e in gen] print(nums) try: v = next(gen) except Exception as e: print(\u0026#34;failed\u0026#34;) Generators using the Yield keyword As seen above, we can implement a generator manually using a class. However, this requires a lot of boilerplate and somewhat obfuscates what the generator is actually doing when the internal state is more complex than a few integers.\nHowever, Python can generate generator instance for us directly from function code when we use the yield keyword! Let\u0026rsquo;s implement our Fibonacci number generator using yield.\ndef fibonacci_generator(n1=0, n2=1, max_iters=100): for i in range(max_iters): sum_ = n1 + n2 n1 = n2 n2 = sum_ yield sum_ Looking at the code above, it is already much simpler and clearer than the class based example. When using yield in a function like this, Python will automatically turn the function into a generator instance, while the yield keyword will act somewhat like a return statement. More specifically, when next(generator) is called, the function will run as expected until it encounters the yield keyword, the value that was yielded is returned to the caller, and the function pauses until the called invoked next(generator) again.\nWe can examine the generator and we see that the behavior is identical to our class based example:\ngen = fibonacci_generator(max_iters=10) nums = [e for e in gen] print(gen) print(nums) try: v = next(gen) except Exception as e: print(\u0026#34;failed\u0026#34;) Example of using a generator to process data structures Now that we\u0026rsquo;ve seen the details of implementing and invoking generators, we can take a look at an example of implementing a generator to traverse a data structure, while implementing the logic separately.\nLet\u0026rsquo;s say that we have some data stored in a binary tree. Any logic that we want to perform on the tree would involve implementing our \u0026ldquo;business\u0026rdquo; logic and our traversal logic in the same place. Alternatively, we can implement a generator that will traverse the tree node by node, and yield the value at each step. We can them implement sum, min and max operations efficiently, without needing access to the internals of the traversal.\nclass Node: def __init__(self, val, l, r): self.val = val self.l = l self.r = r def traverse_tree(root): yield root.val if root.l is not None: for e in traverse_tree(root.l): yield e if root.r is not None: for e in traverse_tree(root.r): yield e if __name__ == \u0026#39;__main__\u0026#39;: a = Node(1, None, None) b = Node(2, None, None) c = Node(4, None, None) d = Node(8, None, None) e = Node(-5, None, None) a.l = b a.r = c b.l = d c.l = e all_vals = [e for e in traverse_tree(a)] print(all_vals) max_ = a.val min_ = a.val for val in traverse_tree(a): if val \u0026gt; max_: max_ = val if val \u0026lt; min_: min_ = val print(max_, min_) Coroutines Coroutines share a lot of similarities with generators, but they provide a few extra methods and a bit of a difference in how the yield keyword is used. In essence, coroutines consume values sent by the caller, instead of returning values to the caller. In terms of technical details, the main differences are:\nCoroutines use send(val) instead of __next__(). The coroutine will then have access to the value sent. Coroutines need to be “primed”. That means you need to initialize it properly before you can start using it (this will raise an error) Like generators, coroutines are suspended on yield keyword, This is can lead to unintuitive behavior if not expected Let\u0026rsquo;s implement a simple coroutine that accepts values and prints them. The key change here is that we will access the value sent by the caller with val = (yield).\ndef simple_coroutine(max_iters=5): for i in range(max_iters): print(f\u0026#39;before yield i = {i}\u0026#39;) val = (yield) print(f\u0026#34;after yield, val = {val}, i = {i}\u0026#34;) To use the coroutine, we need to \u0026ldquo;prime\u0026rdquo; it by either calling coroutine.send(None) or next(coroutine) (these two statements are equivalent). Once the coroutine has been primed, we can iterate over it in the same manner as a generator, with the difference that coroutine.send(val) is used in place of next(generator). Coroutines will even fail the same way as generators if exhausted.\ncoroutine = simple_coroutine() # need to prime the coroutine print(\u0026#39;before priming\u0026#39;) next(coroutine) print(\u0026#39;before send\u0026#39;) coroutine.send(\u0026#39;Dummy val a\u0026#39;) print(\u0026#39;main thread after a\u0026#39;) coroutine.send(\u0026#39;Dummy val b\u0026#39;) print(\u0026#39;main thread after b\u0026#39;) coroutine.send(\u0026#39;Dummy val c\u0026#39;) print(\u0026#39;main thread after c\u0026#39;) coroutine.send(\u0026#39;Dummy val d\u0026#39;) try: coroutine.send(\u0026#39;Dummy val e\u0026#39;) except: print(\u0026#34;failed\u0026#34;) The Yield keyword The yield keyword seems to behave unintuitively, but we can break down what exactly it does to understand the underlying model.\nFunction will run until it encounters yield keyword. The function is then suspended If you yield value, then the value is returned to the caller of send(…) or next() The function will then wait until the next time send(…) or next() is called. If a value is sent, you can access it by value = (yield) Additionally, We can combine generator and coroutine syntax into send_val = yield return_val. This implies that we can have objects which will be both generators and coroutines.\nCo-Generators I like to call objects that are both generators and coroutines, \u0026ldquo;Co-Generators\u0026rdquo; as it helps disambiguate how the object should be interacted with. We can now implement a co-generator which will both accept and yield values.\nWhen dealing with co-generators, it is key to understand the statement sent_val = yield return_val executes in two distinct stages. Upon the next(cogenerator) or cogenerator.send(None), the function will execute up until the yield val statement, which will immediately return the value to the caller. The function will then be suspended until the first call of cogenerator.send(some_val), which will take the value, pass it into the function and will be assigned to sent_val. This means that you can have some external code run after yield val but before val = (yield)!\nBelow we can see an example of a co-generator that sends and yields values, with several print statements that will execute between both steps of the yield evaluation.\ndef complex_cogenerator(max_iters=5): print(\u0026#39;start of cogenerator\u0026#39;) for i in range(max_iters): print(f\u0026#39;start of loop, i={i}\u0026#39;) val = yield i print(f\u0026#39;end of loop, i={i}, val={val}\u0026#39;) print(\u0026#39;end of cogenerator\u0026#39;) yield None if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#39;start of main\u0026#39;) co_gen = complex_cogenerator() print(\u0026#39;after cogenerator creation\u0026#39;) v = next(co_gen) print(f\u0026#39;after cogenerator priming, v={v}\u0026#39;) while v is not None: print(f\u0026#39;main thread before send, v={v}\u0026#39;) v = co_gen.send(\u0026#39;Dummy val a\u0026#39;) print(f\u0026#39;main thread after send, v={v}\u0026#39;) When running this example, we will note a few things:\nno logic runs when complex_cogenerator() is called. In fact this function behaves like an initializer, rather than an actual function. The co-generator needs to be primed before it can be used. But after the priming, we can iterate over the co-generator by using a while loop. The order of execution of the print statements is non-obvious, but makes sense when accounting for the two-step execution of the a = yield b statement. Conclusion In this article, we took a look at generator classes, generators using yield, coroutines using yield and how to combine generators and coroutines into co-generators.\nHopefully you will be able to leverage this knowledge to build better abstractions around you data structures for more flexible, robust and performant code.\n","permalink":"https://philliams.github.io/posts/python_posts/generators_coroutines/","summary":"Generators and Coroutines are very powerful tools in Python that can help simplify logic, speed up data-intensive programs or provide flexible and re-useable APIs. In this post, we will explore three main concepts in Python : Generators, Coroutines and Cogenerators.\nGenerators Generators in Python are objects that contain some sort of internal state, and know how to produce the \u0026ldquo;next\u0026rdquo; value in a sequence.\nBefore we talk about what generators are, we should talk about what problems they can help solve!","title":"Generators and Coroutines in Python"},{"content":"One of the big issues in Machine Learning research is reproducibility. In fields like biology that have many experimental variables and uncertainties, it is expected that results may be difficult to reproduce due to small sample sizes and the inherent complexity of the research. However, there is no excuse for Machine Learning research to be anything but trivial to reproduce.\nIn this blog post, I will be demonstrating ways to make the software environment used for an experiment easily replicated as well as ways to add testing as a part of the experimental process to catch errors that might threaten the integrity of the results. Additionally, I will show how both of these elements can be automated, to remove any human error from the equation. Ideally, the entire experimental setup should be automated, such that it can be replicated from a handful of commands.\nThe first step to reproducible results is being able to precisely replicate the dependencies of the project. More often than not, replicating the data is straightforward, as the datasets are often published online as is. The main challenge is replicating the software environment - such as the libraries and versions - used in the experiments.\nIn this article, we will be discussing the use of Conda and make to automate the setup and teardown of python environments to allow for easy replication of the Python environment. We will be also discuss how we can use make and pytest to run unit tests, monitor code coverage and finally run our experiments.\nThe main idea is that by pre-emptively adopting certain tools and automating our experiment workflow, we can produce high-quality results that can be replicated by anyone with Conda and Make installed.\nPrerequisites This guide is specific to windows 10, however most of the steps are applicable in Linux or have direct counterparts.\nTools:\nChocolatey - used to install Make (use apt-get instead of chocolatey on Linux) GNU Make - Install via choco install make on windows Conda - Used for creating Python environments Environment setup and teardown Conda is one of many tools (one popular alternative is pipenv) that can be used to create Python virtual environments. Virtual environments are used so that you can manage several independent python installations, each with their own versions of installed libraries, without conflict on a single machine. This is great for managing your own project dependencies as well as avoiding cross-contamination and conflicting packages when working on more than one code-base.\nWith Conda, you can specify your environment in a dependencies file. You can then create a Python environment directly from a configuration file and have all of the specified libraries automatically installed. As such, if we put all of the library versions we need in this environment file, we can not only recreate or update our environment as needed, but anyone who wants to replicate our setup can do so directly from the environment file.\nHere is an example of a Conda environment file (named env.yaml) that installs pandas, numpy, scikit-learn and pytorch for Python 3.8:\nname: my_env # your env name channels: - conda-forge # where conda pulls packages from - pytorch dependencies: # all of your library versions - python=3.8 # your python version - numpy=1.20.* - pandas=1.2.* - scikit-learn=0.24.* - pytest - pytorch - torchvision - torchaudio - cudatoolkit=10.2 To create and use the environment, run the following commands in the shell/terminal:\nconda env create -f .\\env.yaml - This will create the environment and install dependencies defined in env.yaml file. conda activate my_env - This will enable your environment in the current shell. If you type python, you should see Python 3.8 as well as be able to import pandas and numpy (among other packages) Thus, if you define the required libraries in your environment file and run all of your experiment code from that environment, anyone who wants to replicate your results can re-create the environment trivially using a single command.\nNote : if there are any issues with an environment, it can be deleted with the command conda env remove -n my_env.\nDeterminism and validation The next component of reproducible (and correct) research is making sure that the experiment code is deterministic and does not contain any significant bugs or errata. In this context, determinism means that if you run the same code twice, it should produce the same result both times. For example, this might mean that any random number generators are seeded such that if you are randomly shuffling your data, it will be randomly shuffled the same way, every time.\nOne of the best ways to ensure that your experiment is deterministic is to simply run it several times, and check if produces the same result. In fact, you could consider testing for determinism as a part of testing the greater validity of the entire experiment.\nThis leads to my next point about reproducible research : testing. There are many different ways of testing code, and many paradigms that one can read about. A good starting point is to at least have unit tests for your code. A unit test is essentially a small, self contained test that will execute a small number of functions or methods and test that it behaves as expected. Obviously this is not sufficient to guarantee that the program is error free, but having a comprehensive set of unit tests that cover the expected cases of your program goes a long way to proving that your program is valid. Additionally, you can have a set of tests that will run your entire experiment multiple times and compare the results to ensure determinism. This way, you can run the tests whenever making a significant change to the code to guarantee that no errors or non-determinism have crept into the experimental setup.\nIn practice, unit tests and code coverage can be implemented using the pytest and pytest-cov libraries. Running the command python -m pytest --cov=unittests will run all of the unit tests in the unittests folder as well as check the code coverage. Code coverage is an interesting metric, since it can help you catch any parts of the experimental code that aren\u0026rsquo;t adequately tested. This can help detect and resolve breaking issues early.\nMakefiles and Automation The final element of a reproducible code setup is to automate everything so that small variations in commands don\u0026rsquo;t influence the final results or lead to divergent experimental environments. Make is a great tool for lightweight automation. In essence, a makefile defines a set of procedures that can easily be invoked, as well as store some variables for the experimental setup. To tie the automation back to the dependencies and unit tests, we can implement a makefile that will allow us to delete old environments, re-create a fresh environment unsing Conda, run all of the unit tests using pytest and then run the experiment proper.\nHere is a sample implementation of a makefile saved in your project directory with the file name makefile:\nenv_file ?= ./env.yaml # this is my env_file path env_name ?= my_env # this is my env name env: conda env remove -n ${env_name} # delete old env conda env create -f ${env_file} # create new one from env.yaml test: python -m pytest unittests/ # run all the unit tests run: python -m main.py # run my main program # this combines test and run full: test run By running the command make env, my old environment will get deleted and then a fresh environment will be created to specification from my env.yaml file using Conda. Next, I can run all of my unit tests with the command make test, this will give me a report of which tests fail and which tests pass, in addition to reporting the percentage of my code that is tested. Finally, I can run my main experiment script with the command make run. I can also combine multiple commands together as can be seen in the make full command, which will run all of my tests, and if they pass it will run my experiment.\nConclusion In conclusion, by implementing my research code using Conda, Make and pytest, I can make my results easy to reproduce by providing a full specification of the dependencies, as well as increase the confidence that the code is correct by testing the code regularly for determinism and errors as a part of my experimental process.\nThe task of reproducing or validating my code has gone from a complex process to a sequence of three simple steps:\nmake env conda activate my_env make run We\u0026rsquo;ve shown that two tools ( make and conda ) can be used to automate the reproduction of results, however any similar setup with different technologies is equally valid. The key takeaway is that the availability of tools and the ease with which code can be automated means that there is no excuse for Machine Learning research to not provide clean code and an automated methodology as a part of the publishing process.\nMany papers have been retracted due to errors that should have been caught early on, and many more have results that can never truly be verified since either the code is not available or there are severe gaps in the methodology to properly re-run the experiment. This is unacceptable and we should all do our part to adopt tool for sharing and reproducing each others results, since the results contained in papers that cannot easily be reproduced are at best suspect and at worst useless.\n","permalink":"https://philliams.github.io/posts/devops/reproducible_ml_1/","summary":"One of the big issues in Machine Learning research is reproducibility. In fields like biology that have many experimental variables and uncertainties, it is expected that results may be difficult to reproduce due to small sample sizes and the inherent complexity of the research. However, there is no excuse for Machine Learning research to be anything but trivial to reproduce.\nIn this blog post, I will be demonstrating ways to make the software environment used for an experiment easily replicated as well as ways to add testing as a part of the experimental process to catch errors that might threaten the integrity of the results.","title":"Reproducible software in research using Python - Part 1"},{"content":"Monads are a super interesting and useful design pattern often seen in functional programming languages such as Haskell. That being said, it is pretty simple to implement our own monads in Python.\nWhen talking about Monads, there are three \u0026ldquo;main\u0026rdquo; things that I use to describe in practical terms what a Monad is and what it does:\nMonads are essentially containers for values. In other words, you will have a Monad that will contain some arbitrary value or variable. You can turn a regular variable into a Monad by taking whatever value you want and \u0026ldquo;wrapping\u0026rdquo; it with your Monad implementation. You need to have a way to apply functions and transformations to Monads to get a new Monad out. We\u0026rsquo;ll talk a little bit more about what the exact terminology is used for the various properties of Monads as well as the specific implementation, but first let\u0026rsquo;s work through a very simple Monad called the Maybe Monad.\nThe Maybe Monad The Maybe Monad is named the way that it is, since it is a simple Monad that can either contain a value or not (maybe it has a value and maybe it doesn\u0026rsquo;t ).\nWhen implementing a Monad (in this case the Maybe Monad), the first thing that we need to have is a way of turning - or \u0026ldquo;wrapping\u0026rdquo; - a value into an instance of the Maybe Monad. The function that does this is called the unit function. In our case we will be implementing the Monad as a class and as such our unit function will actually be the initializer.\nclass MaybeMonad: def __init__(self, value: object = None, contains_value: bool = True): self.value = value self.contains_value = contains_value As you can see in the code snippet above, the first element of our Python Maybe Monad implementation is the unit function in the form of the initializer. In our instance, we will keep track of our inner value self.value as well as a flag to indicate if our Maybe Monad instance contains a value or not self.contains_value (this is needed so that it is still valid if self.value is None or False, as an alternative to checking if self.value).\nNow we have a very simple class that may contain a value or not, we need a way to apply functions to the Monad. Enter the bind function. The bind function is a method on our Monad that takes another function, applies that function to the inner value inside of the Monad and returns a new Monad with the result. The key here is that all of the logic relevant to the behavior of the Monad is implemented in the bind function. In our case, the logic for handling values/exceptions will be the main logic implemented in the bind function, but for other Monad implementations such as lazy evaluation or complex error handling, you might implement different behavior in the bind function.\nfrom collections.abc import Callable class MaybeMonad: def __init__(self, value: object = None, contains_value: bool = True): self.value = value self.contains_value = contains_value def bind(self, f: Callable) -\u0026gt; \u0026#39;MaybeMonad\u0026#39;: if not self.contains_value: return MaybeMonad(None, contains_value=False) try: result = f(self.value) return MaybeMonad(result) except: return MaybeMonad(None, contains_value=False) As you can see above, our MaybeMonad class now has a bind method. The behavior of this Monad is to apply a function to the inner value, and either return a new Monad with the result, or a Monad with no inner value with the self.contains_value flag set to False. Using this Monad, we can now apply a bunch of operations that may or may not fail and simply check if it contains a value at the end of our sequence of operations.\nLet\u0026rsquo;s look at how we might use this Maybe Monad to compute a bunch of operations to a simple numerical type:\nimport numpy as np from maybe_monad import MaybeMonad value = 100 m1 = MaybeMonad(value) print(m1.value) # 100 print(m1.contains_value) # True m2 = m1.bind(np.sqrt) print(m2.value) # 10.0 m3 = m2.bind(lambda x : x / 0) print(m3.contains_value) # True print(m3.value) def exc(x): raise Exception(\u0026#39;Failed\u0026#39;) m4 = m3.bind(exc) print(m4.contains_value) # False In the script above, we can see how we produce the Monad instance m1 from value, we can inspect inside the Monad to see the value, we can also check the flag to see if there is in fact a value inside of the Monad. Next we can bind another function to m1 to get m2, once again we get a new Monad which happens to have a value inside of it. Finally, we try to bind a function to m2 which raises an exception. As a result we obtain a new Maybe Monad m3, which has no value inside of it and who\u0026rsquo;s self.contains_value flag is set to False (since the function failed). Even in this simple example, we can see that Monads are pretty useful, since now we can apply a bunch of functions which may raise an exception, but without needing to have a try-catch block around every function call. All of the try-catch logic is captured inside of the bind method, reducing the amount of repeated code and you to apply the logic of your Monad to any arbitrary code without having to redo any of the work.\nNext we\u0026rsquo;ll build upon this concept and introduce the FailureMonad which will be an extension of the MaybeMonad that provides a bit of additional functionality.\nThe Failure Monad MaybeMonad is pretty useful, since we can use it as a way to sequence several function calls that might raise exceptions, and then pull the value out at the end if no exceptions occurred, but what if we wanted a little more information about where and how the function failed? To do this, we could implement a little more logic in the bind method to keep track of things like the stack trace and the input parameters as well as allow for arguments and keyword arguments to be passed in:\nfrom collections.abc import Callable from typing import Dict import traceback class FailureMonad: def __init__(self, value: object = None, error_status: Dict = None): self.value = value self.error_status = error_status def bind(self, f: Callable, *args, **kwargs) -\u0026gt; \u0026#39;FailureMonad\u0026#39;: if self.error_status: return FailureMonad(None, error_status=self.error_status) try: result = f(self.value, *args, **kwargs) return FailureMonad(result) except Exception as e: failure_status = { \u0026#39;trace\u0026#39; : traceback.format_exc(), \u0026#39;exc\u0026#39; : e, \u0026#39;args\u0026#39; : args, \u0026#39;kwargs\u0026#39; : kwargs } return FailureMonad(None, error_status=failure_status) We can then use FailureMonad to apply a sequence of functions to a value (including args and kwargs) and then print out some information about the exception such as the traceback and the arguments passed:\nimport numpy as np from failure_monad import FailureMonad def dummy_func(a, b, c=3): return a + b + c def exc(x): raise Exception(\u0026#39;Failed\u0026#39;) value = 100 m1 = FailureMonad(value) print(m1.value) # 100 m2 = m1.bind(np.sqrt) print(m2.value) # 10.0 m3 = m2.bind(dummy_func, 1, c=2) print(m3.value) # 13.0 m4 = m3.bind(exc) print(m4.value) # None print(m4.error_status) # {\u0026#39;trace\u0026#39; : ..., \u0026#39;args\u0026#39; : (...,), \u0026#39;kwargs\u0026#39; : {...}} As you can see above, we can use a Monad to not only handle exceptions in a sequence of operations gracefully, but it can also keep track of useful metadata about exceptions so that you can have all of the logic for handling various types of exceptions in a single spot (as opposed to spread out across you program). This is super useful in the context of a production system where you might have different modes of failure (memory issues, timeouts, networking problems, etc.):\nm = FailureMonad(...) m = m.bind(func_1) m = m.bind(func_2) m = m.bind(func_3) if m.error_status: e = m.error_status[\u0026#39;exc\u0026#39;] if isinstance(e, ...): do_something() elif isinstance(e, ...): do_something_else() else: do_thing() So far you might be thinking \u0026ldquo;Monads are useful, but basically you\u0026rsquo;re just passing around some metadata in a fancy way\u0026rdquo;. This is not the only use-case for Monads, next we\u0026rsquo;ll be looking at a Lazy Monad to that Monads can also modify the behavior of your program in really useful ways beyond simple book-keeping operations.\nThe Lazy Monad The last example we\u0026rsquo;ll look at is the LazyMonad. For the LazyMonad we are going to implement a Monad that will lazily evaluate all of functions bound to it. Hopefully this will illustrate the value of Monads beyond simple error catching, since now we can take arbitrary code, and turn it into a lazily evaluated pipeline.\nFirst we will implement the LazyMonad itself, this will be accomplished by internally storing a function, and producing a new Monad which will return a function which itself will return the value of the function computed by the previous Monad(s):\nfrom collections.abc import Callable from typing import Dict import traceback class LazyMonad: def __init__(self, value: object): if isinstance(value, Callable): self.compute = value else: def return_val(): return value self.compute = return_val def bind(self, f: Callable, *args, **kwargs) -\u0026gt; \u0026#39;FailureMonad\u0026#39;: def f_compute(): return f(self.compute(), *args, **kwargs) return LazyMonad(f_compute) In the code above, we can see that the bind operation will return a new Monad as with the other examples, but contrary to the other examples the inner value self.compute is actually a Callable which will compute all the previous steps when needed. We can then use our LazyMonad to demonstrate that arbitrary code can be lazily evaluated without needing to have any awareness of the inner workings of the LazyMonad:\nimport numpy as np from lazy_monad import LazyMonad def dummy_func1(e): print(f\u0026#39;dummy_1 : {e}\u0026#39;) return e def dummy_func2(e): print(f\u0026#39;dummy_2 : {e}\u0026#39;) return e def dummy_func3(e): print(f\u0026#39;dummy_3 : {e}\u0026#39;) return e print(\u0026#39;Start\u0026#39;) value = 100 m1 = LazyMonad(value) print(\u0026#39;After init\u0026#39;) m2 = m1.bind(dummy_func1) print(\u0026#39;After 1\u0026#39;) m3 = m2.bind(dummy_func2) print(\u0026#39;After 2\u0026#39;) m4 = m3.bind(dummy_func3) print(\u0026#39;After 3\u0026#39;) print(m4.compute()) print(\u0026#39;After Compute\u0026#39;) In the example above, we can see that none of the functions run during the .bind(...) call, instead everything is calculated when .compute() is called.\nThe LazyMonad should give an indication of just how powerful Monads can be when used properly. You can essentially encode whatever logic, no matter how complex, in the bind method, and subsequently get the benefit of that logic on any arbitrary code by simply wrapping your variables with the given Monad.\nConclusion Monads are a very interesting and powerful design pattern in Functional languages that can be applied in Python to enable things such as elegant error handling or turning your code into lazily evaluated pipelines with no changes to the code itself.\nThere are many theoretical Computer Science aspects to Monads, but the key is that you need two main operations:\nUnit : Take a value and turn it into a Monad Bind : Take a Monad, apply a function to it and return a new Monad That being said, there are some really awesome resources for anyone looking to read more into the theory behind Monads such as Eric Lippert\u0026rsquo;s blog or Adit Bhargava\u0026rsquo;s \u0026ldquo;Functors, Applicatives, And Monads In Pictures\u0026rdquo;.\n","permalink":"https://philliams.github.io/posts/functional_programming/python_monads/","summary":"Monads are a super interesting and useful design pattern often seen in functional programming languages such as Haskell. That being said, it is pretty simple to implement our own monads in Python.\nWhen talking about Monads, there are three \u0026ldquo;main\u0026rdquo; things that I use to describe in practical terms what a Monad is and what it does:\nMonads are essentially containers for values. In other words, you will have a Monad that will contain some arbitrary value or variable.","title":"Monads in Python"},{"content":"In python, you may have seen some syntax that looks a little like this:\n@something def foo(): pass The @something statement is called a decorator and is syntactic sugar allow you to concisely implement and use interesting concepts from functional programming called \u0026ldquo;Higher-order Functions\u0026rdquo; and \u0026ldquo;currying\u0026rdquo;. Before we get into how and why we might use decorators in Python, let\u0026rsquo;s talk about functional programming, Higher-Order Functions and currying.\nCurrying Currying is a technique for taking a function that takes multiple inputs, and converting it into a sequence of single-input functions. Let\u0026rsquo;s look at a simple function that computes the sum of 3 numbers, and then curry it.\ndef sum(x, y, z): return x + y + z print(sum(1, 2, 3)) If we were to curry the function, it would look something like this:\ndef sum(x): def _sum(y): def __sum(z): return x + y + z return __sum return _sum print(sum(1)(2)(3)) There are a couple of things worth noting about this curried function:\nThe inner function(s) have access to the scope of the outer function(s) The functions themselves return functions To properly execute it, you need to do a bunch of successive function calls Currying is not directly related to decorators, but it leads nicely into the next import concept to grasp when dealing with decorators : Higher-Order Functions.\nHigher-Order Functions Higher-Order Functions are functions that take other functions as inputs and return a new function as an input. This is somewhat different to the curried functions from earlier, as they take in integers and return functions, but the concepts are similar.\nHigher-Order Functions are super useful for modifying the behavior of a function from outside the function itself. Let\u0026rsquo;s look at an example where we use a Higher-Order Function to retry a function that throws an exception:\ndef throws_exc(): print(\u0026#39;this function will raise an exception!\u0026#39;) raise Exception(\u0026#39;This Failed\u0026#39;) def higher_order_retry(f): def _retry(): try: f() except: f() return _retry new_function = higher_order_retry(throws_exc) new_function() # this will print twice! In the code snippet above the higher_order_retry function is in fact a Higher-Order Function. It will take in a function pointer f and will return a new function pointer that wraps the original one in a try-catch block.\nHigher-Order Functions are clearly powerful, since I can now write my try-catch logic once, and use it everywhere I want in my code! The main issue with this type of approach is that the higher_order_retry(f)() syntax is pretty cumbersome and we need to add this verbose function call anywhere that we want to modify the behavior of f.\nDecorators Enter decorators. Decorators are Python syntactic sugar that allows us to apply Higher-Order Functions to a function definition in a single line of code, and have that higher order logic be automatically applied anywhere that the function is used.\nFirst, we implement our decorator in the form of a function, here the inner block __retry accepts any number of args and kwargs and passes them along to the function being wrapped.\ndef retry_decorator(f): def __retry(*args, **kwargs): try: return f(*args, **kwargs) except: return f(*args, **kwargs) return __retry Next, we can use the @decorator syntax to apply our decorator to any function we care about:\n@retry_decorator def throws_exception(a, b=None): print(f\u0026#39;a={a}, b={b}\u0026#39;) raise Exception(\u0026#39;This function failed\u0026#39;) throws_exception(1, b=2) # this will print twice! This is equivalent to the following implementation without using decorators:\ndef throws_exception(a, b=None): print(f\u0026#39;a={a}, b={b}\u0026#39;) raise Exception(\u0026#39;This function failed\u0026#39;) throws_exception = retry_decorator(throws_exception) throws_exception(1, b=2) # this will print twice! Or this implementation if you want to explicitly use the Higher-Order Function every time:\nretry_decorator(throws_exception)(1, b=2) Now that we know what decorators are, how they work and how to implement them, we can finally talk about why you might want to use decorators.\nDecorators are a really awesome way to modify the behavior of the function without modifying the code inside of the function. This allows you to solver certain types of problems in a generic and re-useable way.\nTakeaways Decorators are useful whenever you want to modify the behavior of some code in a generic way from outside the function itself. A few common situations where decorators are often used would be:\nRetrying a flakey operation (for example exponential back off for network communication). By implementing a retry decorator, you can have all of your network communication use the same retry strategy without duplicate or repeated code.\nLogging/debugging code can be made much easier by implementing decorators that print out all of the inputs to a given function, allowing you to easily spot check every place where a function is called while only needing to add a single line of code at the function definition.\nAdvanced functional programming techniques such as Monads also benefit from the use of decorators and can allow you to use exotic and powerful design patterns while keeping the code clean and concise.\nIn conclusion, by using decorators, you can reduce repeated or duplicated code, make your software cleaner and easier to maintain, make logging and debugging easier as well as seamlessly integrate more exotic design patterns without adding visual clutter or verbosity.\n","permalink":"https://philliams.github.io/posts/python_posts/python_decorators/","summary":"In python, you may have seen some syntax that looks a little like this:\n@something def foo(): pass The @something statement is called a decorator and is syntactic sugar allow you to concisely implement and use interesting concepts from functional programming called \u0026ldquo;Higher-order Functions\u0026rdquo; and \u0026ldquo;currying\u0026rdquo;. Before we get into how and why we might use decorators in Python, let\u0026rsquo;s talk about functional programming, Higher-Order Functions and currying.\nCurrying Currying is a technique for taking a function that takes multiple inputs, and converting it into a sequence of single-input functions.","title":"Decorators in Python"},{"content":"One of the more interesting Machine Learning models is the Neural Network. A Neural Network is a highly non-linear mathematical model that can be fitted to very complicated datasets, from image classification to text translation. In this blog post, we’ll be implementing our own simple Neural Network library in python, then test how our model performs through a practical example on an image classification dataset.\nWhat is a Neural Network? There are several types of Neural Networks, however we will be examining the simplest variant : a simple Feed-Forward Neural Network. In a Feed-Forward Neural Network, there are 3 main components :\nNeurons Activation Functions Layers A neuron is a simple non-linear entity, it can be defined by the two following equations :\n$$ z = b + \\sum x_i \\cdot w_i $$\n$$ y = f(z) $$\nIn this notation, the $x_i$ value indicates the ith value of the input vector $x$, while the $w_i$ value is the ith value of the weight vector $w$. The $b$ term is called the bias and and allows the neuron to model an offset, while the weights allow the neuron to model how quickly the output should change in response to a change in the inputs. Finally, the $f(z)$ function is an arbitrary non-linear function typically chosen from a catalogue of standard functions.\nActivation Functions The output of the neuron is defined as some function $f(z)$ of the weighted sum, but what is is it? $f(z)$ is called the activation function and can be chosen from a selection of commonly used functions (or you can implement your own custom function as long as it is continuous and differentiable). Some common activations functions are the Sigmoid function (also called logistic function), the Hyperbolic Tangent function or the Rectifier function (also called ReLu).\nThe Sigmoid function is given by:\n$$ f(z) = \\frac{1}{1+e^{-z}} $$\nThe Hyperbolic Tangent function is defined as follows :\n$$ f(z) = \\frac{e^{2z} - 1}{e^{2z} + 1} $$\nThe Rectifier function (ReLu) is given as follows:\n$$ f(z) = max(z, 0) $$\nAn interesting fact about the Rectifier Function is that the derivative is not defined at 0, since the function is piecewise, however in practice we can simply decide what side of the piecewise function to use when calculating the derivative.\nNow that we have the defining equations for the neurons and the activation functions, we can arrange the neurons into layers to form an actual Neural Network. A Neural Network can be made up of a single neuron or thousands of them, but typically conforms to the following architecture :\nIn the diagram above, the input layer represents the raw numerical input vector, each circle in the input layer represents a single input dimension, so if there is 16 circles in the input layer, we can imagine that the neural network will receive a vector of 16 values as input. We call these input values features.\nThe lines represent the connections between the neurons, so if there is a line between two neurons, the output of the neuron on the left will be taken as an input to the neuron on the right.\nFor the hidden and output layers, each circle represents a single neuron, and as you can see, all the outputs from one layer are fed in as inputs to all the neurons in the next layer.\nThe number of layers, and the number of neurons in each layer is determined experimentally or inspired by heuristics, however the input and output layer sizes are usually determined by the available data. For example, you may try to predict a set of 3 values based on an input vector with 32 features.\nWith the structure of the Neural Network defined, and the functions within the neurons given, we can establish how to actually solve for the weights that will give the best performance.\nGradient Descent and Backpropagation There are two algorithms commonly used to solve for the weights of a neural network: Gradient Descent and Backpropagation. First let’s look at the output neurons of the network.\nTo understand Gradient Descent, we need to choose a cost function $C$ (also called error $E$). This cost function should decrease as performance gets better, this way we can frame the problem of training a Neural Network as optimizing the weights of each neuron in the network to minimize the cost (or error) of the network on some training data. For our network we will be using the mean squared error (MSE), which is defined as follows :\n$$ E = \\frac{1}{2}(y - \\hat{y})^2 $$\nThe further away we are from the right answer, the larger our error $E$ is. Note that $y$ is the true value for a given data point and $\\hat{y}$​ is the output of our model for that same input. We can now find the derivative of the error with regard to the model output, so that we can figure out how the error changes with the output of the model:\n$$ \\frac{\\partial E}{\\partial \\hat{y}} = y - \\hat{y}$$\nBut we what we really want to know is how the cost changes with a change in the weights, since that would allow us to optimize the weights directly. The first step is to go one step further with our derivative (using the chain rule) and find how $E$ changes with the $z$ value of the output neuron:\n$$ \\frac{\\partial E}{\\partial z} = \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} $$\nFinally, we can go one step further and derive the error such that we get the derivative of the error relative to the weight of the output neuron:\n$$ \\frac{\\partial E}{\\partial w_i} = \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i} $$\nBut from earlier, we know that $\\hat{y}=f(z)$ and $ z = b +\\sum w_i \\cdot x_i $​, so we by replacing the partial derivatives in the previous equation, we can get the actual derivative of the error relative to the weights and bias:\n$$ \\frac{\\partial E}{\\partial w_i} = (y - \\hat{y}) \\cdot f\u0026rsquo;(z) \\cdot x_i$$ $$ \\frac{\\partial E}{\\partial b} = (y - \\hat{y}) \\cdot f\u0026rsquo;(z) $$\nSo now that we have the derivative, what do we do? Here, we apply Gradient Descent. Basically, using the derivative we can figure out if an increase or a decrease of the value of the weight will decrease the cost, then we iteratively update the weights to minimize the weight. $\\eta$ is called the \u0026ldquo;Learning Rate\u0026rdquo; and is a constant chosen to adjust the size of the iteration steps. By iteratively updating the weights and biases, we can converge to a local minima which will (hopefully) give us a model with a good prediction accuracy:\n$$ w_{i,t+1} = w_{i, t} + \\eta \\cdot \\frac{\\partial E}{\\partial w_{i, t}}$$ $$ b_{t+1} = b_{t} + \\eta \\cdot \\frac{\\partial E}{\\partial b_{t}}$$\nHowever, in the hidden layers, we don’t know what the desired output of the specific neurone is, this is where backpropagation comes in handy. Basically, we will propagate the errors of the output layer back through the network so that we can calculate the derivatives of the weights relative to the cost for all of the weights.\nFor this, we will need to define another equations. We replace the $\\frac{\\partial E}{\\partial \\hat{y}}$ with:\n$$ \\frac{\\partial E}{\\partial \\hat{y}} = \\sum w_i \\cdot \\frac{\\partial E}{\\partial z_i}$$\nThis tells us that for a node in a hidden layer, the derivative of the error relative to it\u0026rsquo;s weights is equal to the weighted sums of the derivatives of the next layer. The weights used are simply the weights of the nodes in the next layer for that given node.\nWe can apply all the same equations as before when calculating the derivatives as before, we simply need to calculate the initial $\\frac{\\partial E}{\\partial \\hat{y}}$​ of the output layer, then use the derivative equations to propagate the derivatives through the network, then finally update the weights after we’ve calculated the relevant values.\nRecap of the algorithm Here is a recap of the algorithm to implement a simple Neural Network, assuming you have a collection of numerical input vectors and the desired true/false output label:\nCalculate the derivatives of the output neurons relative to the error using an input vector $x$ with a known output $y$ $$ \\frac{\\partial E}{\\partial w_i} = (y - \\hat{y}) \\cdot f\u0026rsquo;(z) \\cdot x_i$$ $$ \\frac{\\partial E}{\\partial b} = (y - \\hat{y}) \\cdot f\u0026rsquo;(z) $$\nPropagate the errors of the last layer to the second last layer, then propagate the errors of the second last layer to the third last layer, etc. where $\\hat{y}$​ is the output of a hidden node $$ \\frac{\\partial E}{\\partial \\hat{y}} = \\sum w_i \\cdot \\frac{\\partial E}{\\partial z_i}$$\nOnce you know the derivatives of all the weights and biases for all the neurons, update them using the following rules: $$ w_{i,t+1} = w_{i, t} + \\eta \\cdot \\frac{\\partial E}{\\partial w_{i, t}}$$ $$ b_{t+1} = b_{t} + \\eta \\cdot \\frac{\\partial E}{\\partial b_{t}}$$\nPython Implementation Now that all the of the theoretical equations have been established, we can actually implement our model and test it on some real world data. For this example, we will be using the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset. You can download a copy of the dataset directly, or you can import it through the Scikit learn dataset module. We are trying to predict if a tumor is bening or malignant with several features such as the radius, symmetry, smoothness and texture.\nfrom sklearn.datasets import load_breast_cancer import numpy import math import random # implementation of sigmoid neurones in a layer class SigmoidLayer: # layer is defined by the input size and the number of neurones def __init__(self, input_size, number_of_neurones): self.input_size = input_size self.number_of_neurones = number_of_neurones # generate the weights as random numpy arrays of small values # different initiliaztion schemes can be used for the weights with varying experimental success self.biases = numpy.random.normal(0., 0.1, number_of_neurones) self.weights = numpy.random.normal(0., 0.1, (number_of_neurones, input_size)) # defined the sigmoid function def sigmoid(self, z): return 1.0/(1.0 + math.exp(-z)) # calculate the output of the neurones as a vector, based on an input vector to the layer def output(self, x): # initialize layer output, output will be of length self.number_of_neurones output_vector = [] # iterate over each neurone in the layer for neurone in range(self.number_of_neurones): # retrieve the bias term for the given neurone z = self.biases[neurone] # calculate the weighted sum and add it to the initial bias term for feature in range(self.input_size): z += self.weights[neurone][feature] * x[feature] # apply the activation function neurone_output = self.sigmoid(z) # add the output of the given neurone to the layer output output_vector += [neurone_output] # return the output of the layer as a vector return numpy.array(output_vector) # calculate the backpropagation step for the layer as well as update the weights, based on the last input, output # and the derivatives of the layer, will output the derivatives needed for the previous layer def backpropagate(self, last_input, last_output, dE_dy, learning_rate): dE_dz = [] # use the dE_dy of the layer to calculate the dE_dz of each neurone in the layer for neurone in range(self.number_of_neurones): # apply the derivative of the sigmoid function neurone_dE_dz = last_output[neurone] * (1.0 - last_output[neurone]) * dE_dy[neurone] # keep track of the derivatives of each neurone dE_dz += [neurone_dE_dz] dE_dz = numpy.array(dE_dz) # use the dE_dz derivative as well as the last input to update the weights and biases # this is the gradient descent step for neurone in range(self.number_of_neurones): # update the bias of each neurone in the layer self.biases[neurone] -= learning_rate * dE_dz[neurone] for feature in range(self.input_size): # calculate the derivative relative to each weight, then update each weight for each neurone self.weights[neurone][feature] -= learning_rate * last_input[feature] * dE_dz[neurone] # calculate the dE_dy derivative to be used by the following layer next_layer_dE_dy = numpy.zeros(self.input_size) # iterate over each neurone for neurone in range(self.number_of_neurones): # iterate over each weight for feature in range(self.input_size): # calculate the derivative using the backpropagation rule next_layer_dE_dy[feature] += self.weights[neurone][feature] * dE_dz[neurone] return next_layer_dE_dy # implement Neural Network using sigmoid layer class SigmoidFeedForwardNeuralNetwork: # we need the number and sizes of the layers, the input size and the learning rate for the network def __init__(self, learning_rate, input_size, layer_sizes): self.learning_rate = learning_rate self.layers = None # initialize each layer based on the defined sizes for layer in range(len(layer_sizes)): # input size of first layer is input size if layer == 0: self.layers = [SigmoidLayer(input_size, layer_sizes[layer])] # input size of every other layer is the size of the previous layer else: self.layers += [SigmoidLayer(layer_sizes[layer - 1], layer_sizes[layer])] # calculate the output of the neural network for a given input layer def predict_on_vector(self, x): # feed the output of each layer as the input to the next layer for layer in self.layers: x = layer.output(x) # return the output of the last layer return x # calculate the outputs of the neural network for a list of vectors def predict(self, X): # calculate the prediction for each vector predictions = [] # make a prediction for each vector in the set for i in range(len(X)): prediction = self.predict_on_vector(X[i]) predictions += [prediction] # return all of the predictions return numpy.array(predictions) def train(self, X, Y, iterations=1): # generate a list of indexes for the training samples training_samples = [i for i in range(len(X))] # do k iterations of training on the dataset for k in range(iterations): # print training progress print(\u0026#34;Epoch : \u0026#34; + str(k)) # randomly shuffle dataset to avoid getting stuck in local minima # random.shuffle(training_samples) # train on each sample for index in training_samples: self.train_on_vector(X[index], Y[index]) # train the Neural Network on a single vector, this training scheme is know as \u0026#34;on-line\u0026#34; training # as the neural network is updated every time a new vector is given def train_on_vector(self, x, y): outputs = [] inputs = [] # iterate over each layer and keep track of the inputs and output for layer in self.layers: inputs += [x] x = layer.output(x) outputs += [x] # calculate the error vector of the output neurones error_vector = x - y # iterate over each layer and apply the backpropagation, starting for i in range(len(self.layers)): index = len(self.layers) - 1 - i # update the weights of the layers and retrieve the next layer\u0026#39;s error vector error_vector = self.layers[index].backpropagate(inputs[index], outputs[index], error_vector, self.learning_rate) # Apply the logistic regression model to the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset X, Y = load_breast_cancer(return_X_y=True) Y = [numpy.array([label]) for label in Y] #split the data into training and testing sets X_test = X[:100] X_train = X[100:] Y_test = Y[:100] Y_train = Y[100:] # Initialize the model model = SigmoidFeedForwardNeuralNetwork(1e-1, 2, [64, 8, 1]) # Train the model on the training set model.train(X_train, Y_train, iterations=25) # calculate the accuracy on the training set predictions = model.predict(X_train) accuracy = 0 for i in range(len(predictions)): if predictions[i][0] \u0026gt;= 0.5 and Y_train[i][0] == 1: accuracy += 1 elif predictions[i][0] \u0026lt; 0.5 and Y_train[i][0] == 0: accuracy += 1 print(\u0026#34;Training Accuracy (%) = \u0026#34; + str(100. * accuracy / float(len(predictions)))) # calculate the accuracy on the testing set predictions = model.predict(X_test) accuracy = 0 for i in range(len(predictions)): if predictions[i][0] \u0026gt;= 0.5 and Y_test[i][0] == 1: accuracy += 1 elif predictions[i][0] \u0026lt; 0.5 and Y_test[i][0] == 0: accuracy += 1 print(\u0026#34;Testing Accuracy (%) = \u0026#34; + str(100. * accuracy / float(len(predictions)))) ","permalink":"https://philliams.github.io/posts/machine_learning_basic/neural_network/","summary":"One of the more interesting Machine Learning models is the Neural Network. A Neural Network is a highly non-linear mathematical model that can be fitted to very complicated datasets, from image classification to text translation. In this blog post, we’ll be implementing our own simple Neural Network library in python, then test how our model performs through a practical example on an image classification dataset.\nWhat is a Neural Network? There are several types of Neural Networks, however we will be examining the simplest variant : a simple Feed-Forward Neural Network.","title":"Implementing Neural Networks in Python"},{"content":"One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.\nWhat is Logistic Regression? First of all, when we talk about Machine Learning, we are really talking about curve fitting. What this means is that we have some numerical input data as well as the numerical output we want, we’ll then use that data to create a mathmatical model that can take in some input data and output the correct values.\nIn the case of Logistic Regression, we take in a vector of numerical values, and we get an output between 0 and 1. This is a specific type of Machine Learning classification. Basically, we want to know if something about the input data is true or false, with 1 corresponding to true and 0 corresponding to false.\nNow that we’ve got that, what is Logistic Regression really? Logistic Regression is defined by two main equations:\n$$ z = \\sum w_i \\cdot x_i $$ $$ y = \\frac{1}{1+e^{-z}} $$\n$x_i$​ is the ith element of our input vector, $w_i$ is the weight of that specific input and $z$ is the weighted sum of the $x$ and $w$ vectors. $y$ on the other hand, is the final output of the Logistic Regression equation and looks like this:\nTraining a Linear Model So now we have an idea of what our model looks like and how it is defined. The next step is to actually train the model by solving for the ww vector. Assuming we have a dataset of vectors xx (all of the same size) and values yy values that we want to predict, we want to find our weight vector ww that will maximize the accuracy of our model and give correct predictions.\nThe are several algorithms that can do this, each having their own pros and cons, such as Gradient Descent or Genetic Algorithms. However, we are going to train our Logistic Regression model using Linear Regression.\nThe linear model used in Linear Regression is given by the following equation:\n$$ y = \\sum b_i \\cdot x_i $$\n$b$ is our weight vector and can be obtained via Ordinary Least Squares:\n$$ b = (X^{T}X)^{-1}(X^{T}Y) $$\nWhen solving for $b$, $X$ is a 2D matrix, each row corresponds to a single input vector, $Y$ is a vector of the known outputs for the corresponding input in the $X$ matrix, $M^T$ and $M^{-1}$ are the matrix operations (for an arbitrary matrix $M$) of transpose and inversion respectively. We will not implement these matrix functions ourselves, but will instead use the built in NumPy functions for ease.\nThis should seem very similar, since it is exactly the same equation for $z$ in the Logistic Regression model, the only difference is that we pass the sum through a sigmoid transformation when performing Logistic Regression.\nBridging Linear and Logistic Regression This is where we can use a clever trick to transform the Logistic Regression problem into a Linear Regression problem. By applying the following function to the true/false (1/0) values of the classification, we can get equivalent values to train a Linear Regression model :\n$$ y = \\frac{1}{1+e^{-z}} $$ $$ z = ln(\\frac{y}{1 - y}) $$\nHowever, if we plug in the classification values of 0 and 1, we will get a domain error when calculating $z$, since we can’t divide by 0 or calculate the log of 0. To solve this, we can simply use values very close to 0 and 1 for our classification output, for example 0.001 and 0.999. This technique is called Label Smoothing.\nSo now, to train our Logistic Regression model, we take the classification output of 1 or 0, add some small constant to avoid numerical errors, train a Linear Regression model on the transformed data, then use the Linear Model and the Logistic function to make predictions on new data.\nRecap of the algorithm Here is a recap of the algorithm to implement Logistic Regression, assuming you have a collection of numerical input vectors and the desired true/false output label:\nUse label smoothing to convert each 0/1 label into 0.001/0.999 to avoid numerical issues.\nConvert the smoothed labels into the linear domain using the following equation, where $y$ is the smoothed label and $z$ is the linear value by using: $z=ln(\\frac{y}{1−y})$\nSolve for the $b$ vector using: $ b = (X^{T}X)^{-1}(X^{T}Y) $, where the $Y$ vector is a list of the smoothed linear $z$ value\nUse the weight vector $b$ and a new input vector $x$ to make a prediction using : $$ z(x) = \\sum x_i \\cdot b_i $$ $$y(x) = \\frac{1}{1 + e^{-z(x)}}$$\nPython Implementation Now that all the of the theoretical equations have been established, we can actually implement our model and test it on some real world data. For this example, we will be using the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset. You can download a copy of the dataset directly, or you can import it through the Scikit learn dataset module. We are trying to predict if a tumor is bening or malignant with several features such as the radius, symmetry, smoothness and texture.\nfrom sklearn.datasets import load_breast_cancer import numpy import math # Define class to implement our logistic regression model class LogisticRegression(): def __init__(self, use_bias=False, smooth=1e-2, cutoff=0.5): # bias determines if we use a bias term or not self.use_bias = use_bias # instance variable for our weight vector self.weights = None # the cutoff is used to determine the prediction, if y(z) \u0026gt;= cutoff, y(z) = 1, else y(z) = 0 self.cutoff = cutoff # the amount of smoothing used on the output labels self.smooth = smooth # will smooth all the labels given the Y vector def smooth_labels(self, Y): y_smooth = [] for i in range(len(Y)): if Y[i] \u0026gt;= 1 - self.smooth: y_smooth += [1.0 - self.smooth] else: y_smooth += [self.smooth] return numpy.array(y_smooth) # calculates y given a single z def logistic_function(self, z): y = 1.0/(1.0 + math.exp(-z)) return y # calculates z given a single y def inverse_logistic_function(self, y): z = math.log(y/(1-y)) return z # convert the labels from 0/1 values to linear values def transform_labels_to_linear(self, Y): for i in range(len(Y)): Y[i] = self.inverse_logistic_function(Y[i]) return Y # use the weights and a new vector to make a prediction def predict_on_vector(self, x): # calculate the weighted sum z = numpy.matmul(numpy.transpose(x), self.weights) prediction = self.logistic_function(z) if prediction \u0026gt;= self.cutoff: return 1 else: return 0 def predict(self, X): # using a bias will add a feature to each vector that is set to 1 # this allows the model to learn a \u0026#34;default\u0026#34; value from this constant # the bias can be thought of as the offset, while the weights are the slopes if self.use_bias: ones = numpy.array([[1.0] for i in range(len(X))]) X = numpy.append(ones, X, axis=1) # calculate the prediction for each vector predictions = [] for i in range(len(X)): prediction = self.predict_on_vector(X[i]) predictions += [prediction] return numpy.array(predictions) # train the model on the dataset def train(self, X, Y): # using a bias will add a feature to each vector that is set to 1 # this allows the model to learn a \u0026#34;default\u0026#34; value from this constant # the bias can be thought of as the offset, while the weights are the slopes if self.use_bias: ones = numpy.array([[1.0] for i in range(len(X))]) X = numpy.append(ones, X, axis=1) # smooth the labels Y = self.smooth_labels(Y) # convert labels to linear Z = self.transform_labels_to_linear(Y) # calculate weights self.weights = numpy.matmul( numpy.linalg.inv( numpy.matmul( numpy.transpose(X), X ) ), numpy.matmul( numpy.transpose(X), Y ) ) # Apply the logistic regression model to the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset X, Y = load_breast_cancer(return_X_y=True) #split the data into training and testing sets X_test = X[:100] X_train = X[100:] Y_test = Y[:100] Y_train = Y[100:] # Initialize the model model = LogisticRegression(use_bias=True) # Train the model on the training set model.train(X_train, Y_train) # calculate the accuracy on the training set predictions = model.predict(X_train) print(\u0026#34;Training Accuracy (%) = \u0026#34; + str(100 * numpy.sum(predictions == Y_train)/ float(len(Y_train)))) # calculate the accuracy on the testing set predictions = model.predict(X_test) print(\u0026#34;Testing Accuracy (%) = \u0026#34; + str(100 * numpy.sum(predictions == Y_test)/ float(len(Y_test)))) ","permalink":"https://philliams.github.io/posts/machine_learning_basic/logistic_regression/","summary":"One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.\nWhat is Logistic Regression? First of all, when we talk about Machine Learning, we are really talking about curve fitting.","title":"Implementing Logistic regression in Python"},{"content":"I’ve gotten a lot of questions on how to get a job as a student. Often students want to buff up their resumes or skills a few weeks before their interviews. Honestly, I don’t think there’s much you can do in less than a few months to improve your chances. Obviously you can review algorithms, practice questions and buff up you knowledge to give you a slight advantage, but if you really want to level up and give yourself the best chance at landing the type of jobs you want, you need to start at least 6 months in advance. That being said, a LOT can be accomplished in a year.\nHow to be a good candidate (6 months to 1 year before you want a job) Before you ever apply to a job, there is a lot that you can do to improve your quality as a candidate and pick up qualifications that will make getting interviews and job offers much easier. These are things that you would do well before applying to a job to set yourself up to be in a good position when you start looking for a job.\nStudent events : Participating in student events such as computer science clubs, hackathons, meetups, etc., is a great start. These are good fun and help you meet interesting people and gain skills. Additionally, including these events on your resume is a great way to demonstrate that you are driven and willing to learn.\nProgramming competitions : There are a ton of competitions that students can participate in. These are a great way to learn as well as meeting interesting people. Additionally winning/competing in competitions is a great way to improve your resume and stand out to recruiters. There are different types of competitions for whatever you are more interested in.\nHackathons : Hackathons are usually 24-48 hour competitions in teams of 3-6 students that commonly focus on quickly throwing together different pieces and often have industry sponsors. There is a lot of value in doing a few hackathons to improve your resume, meet people and learn about that type of coding but I would avoid focusing only on hackathons to the detriment of other events.\nTheoretical Competitions : These competitions are often longer and broken up into various chunks and cover a wide range of domains. Students typically work on very difficult problems over the course of 3-6 hours before moving onto the next problems. The challenges are typically more on the theoretical side and are very different than hackathons. These competitions are often completed in groups of 10-20 students over a long weekend. These are a great way to learn from more experienced students on the team and will expose a whole different side of computer science while looking very impressive on a resume.\nDemo competitions : These competitions are often very open in scope, students can present any project that they have worked on to a panel of judges who determine the prizes based on a number of factors. These competitions are very unique, since you often have many months to work on a project and have no clearly defined goal.\nSide projects : In my opinion, side projects are the best way to learn and improve your resume. Basically, you pick any topic that you find interesting, and come up with an end goal. For example an iOS app, or a machine learning program, or a video game or a website. Really anything. The fun part of this is that in trying to complete an actual project/program, you will unintentionally be exposed to a ton of domain knowledge and will more or less gain “work” experience by working on your own towards a finished product. Additionally, side projects look really good on resumes since they are more or less equivalent to work experience (and can make even a student with no previous work experience look good) as well as demonstrating drive, curiosity and deep domain knowledge.\nMy recommendation would be to participate in one of each, for a few reasons. You will be able to explore a bunch of different sub-domains of computer science and find out what you like, you will meet talented students that you can learn from or that can help you find a job, and you will gain a bunch of experience that will both improve your skills as a computer scientist and make your resume stand out more when applying for jobs. Below is a list of some of my favorite events that I would recommend:\nComputer science games (theoretical competition, both uOttawa and Carleton send teams every year) CUSEC (Conference, career fair and demo competitions, both Carleton and uOttawa fund delegations each year) Hack the north / Major League Hacking (one of the more popular/prestigious hackathon for students) Also look up the various student organizations and meetups on campus, for example the CS club at uOttawa.\nFinding Companies to apply to (6 months before you want a job) The first step is to find companies that are hiring and that you would like to work for. There are a couple ways that you can find out more about various companies:\nConferences / Hackathons Career fairs online website/forums Conferences/Hackathons There are a lot of student run/focused events such as conferences and hackathons. Very often the sponsors of such events are companies that are hiring, and often they are looking for both full time and interns hires. As such, looking for the sponsor list of any Computer Science/Software Engineering related events is a good way to get lists of companies that are hiring.\nAdditionally, these events are a great way to meet recruiters to have an easier time getting an interview. At these events, sponsors will be keeping an eye out for candidates that stand out. Usually the students that we notice are the ones that will show off a good project, win some sort of prize or have a very strong resume. For example, I got my internships through participation in competitions at a student run conference.\nThat being said, recruiters get a lot of resumes at these events and so you need to make sure to have a compelling conversation with the recruiters. A lot of time, students will simply ask about the company or won’t have really much to contribute. A good way to stand out is to have a “game plan”, know what the company does and why you want to work there before talking to the recruiters, that way you can have an actual conversation that hopefully leads to an interview. Another way of standing out is competing in some sort of competition that the sponsors are involved in, and then when talking to the recruiters you can bring up your project/results to see if the recruiters want to give you an interview,\nAlways remember to talk to the recruiters, get their email if possible, then apply online and follow up.\nCareer Fairs Career fairs are good for finding companies that are hiring, but it’s difficult to make an impression on the recruiters or get an interview directly, as such it’s more useful for getting a list of companies to apply to unless you have a very strong resume for some of the companies in particular.\nOnline Websites/Forums These are great places to find jobs and most companies require you to apply online anyways. Communities such as reddit or hacker news are great for finding smaller companies that might not show up at student events. For example hacker news has a monthly post where people share what companies are hiring (typically startups). Dedicating a few days to searching online is a great use of time since you’ll find a lot of companies that you might potentially want to work for.\nLinks:\nHacker news Reddit/r/CSCareerQuestions Reddit/r/CSMajors CUSEC Applying for jobs (3 to 6 months before you want a job) Once you’ve found the companies you want to apply to, there are a few steps before applying:\nWhat job(s) to apply to Writing your resume Cover Letters What jobs to apply to There are a lot of jobs that have a ton of requirements that you may not fulfill (for example knowledge of a specific technology or years of experience). These requirements are important but flexible, don’t apply for a job that you are completely unqualified to do (for example applying for a iOS developer job without any experience in mobile development), but if you fulfill most of the requirements and are only missing a few points or if you feel like you would be a good fit despite not having everything they ask for go ahead and apply anyways. Almost no candidates have all the points that they are asking for and it’s often just a filter to avoid getting overwhelmed by too many applications.\nSo apply for any job that you feel you have some relevant experience/knowledge, even if you don’t meet all of the requirements.\nWriting your resume The most important part of your application is your resume as such you should put most of your effort into making it as good as possible. That being said, there are a lot of different opinions about exactly how to make a resume, but there are a few key points that most recruiters agree on.\nKeep it short (1 or 2 pages): if you have a lot of relevant experience two pages is okay, but most students/new grads can make due with 1 page\nHighlight the key points: a lot of times, students/new grads will cram as much information into their resumes as possible. This is often not a great strategy, since it might make it harder to see the really good parts of your resume. So what you should do is make it so that your best experience/qualifications/awards are visually easy to see/read. It’s better to have your 3 best points very clear than to have 10 points that are not as good. An example of what not to do is adding jobs held in highschool or that are not relevant, when instead you could use that space to highlight industry experience, awards, scholarships or good grades.\nAdding your school courses or projects is good, but avoid adding courses/projects that are expected of a Computer Science student. For example, adding that you took an algorithms course is not great, since we assume that computer science majors take algorithms, but if you took an interesting technical elective or participated in a research project, those are great to add.\nUse some sort of software to make your resume so that it looks good (using templates is fine), i recommend using canva as they have a ton of good free templates.\nIf you have a lot of different experience/projects or whatever that you can put on your resume, you can tailor your resume to each job you are applying to by reformatting and focusing on the projects that are relevant to the job.\nCover Letters Many jobs require that you submit a cover letter. The issue is that many of the hiring managers/recruiters don’t read the cover letter while some do. My advice would be to write a 0.5-1 page letter describing why you want to work there and why you think you would be a good fit (experience, personality, etc.) but if it comes down to it, put more effort into polishing your resume. There are a lot of example cover letters online so use them for inspiration, but make sure that you customize it and make it unique for the jobs that you are applying for.\nIn the interview I would say that there are two main types of interviews. There are interviews that are centered around typical interview questions and data structures and there are interviews around your previous experience/projects.\nAlgorithms interview These interviews focus around asking you a few theoretical questions around algorithms to make sure that you have some knowledge of the domain. IT’s somewhat hard to prepare for these interviews because there are thousands of possible algorithms and questions they could ask. However, books such as “Cracking the coding interview” or websites such as “Leet code” are very good ways to prepare for these types of questions. When in such an interview, don’t try to come up with the best answer right away, try to come up a “brute force” approach that works and then try to improve it from there. Also make sure to say out loud what you are thinking. If you get a brute force solution working, the interviewer will at least know that you understand the question and can come up with a basic solution, and by talking out loud, the interviewer can follow your train of thought. This usually helps the interview get an idea of how you work, where if you just stay silent, the interviewer may assume that you simply don’t understand. Project/Behavioral interview\nThese interviews focus on how you think and your work experience. These interviews are a lot easier in some ways, since there is no right or wrong answer, but at the same time you need to make sure to talk about impressive things that you’ve done so that you can demonstrate knowledge. A common thing is to go over your projects and talk about it. This is a good time to bring up any interesting or challenging problems that you faced and how you overcame them. Don’t over exaggerate, but you can definitely sell yourself at this point by bringing up all the tricky or interesting things that you did/learned/worked on. This is why doing side projects and competitions is good for this type of interview, since you will have a larger bank of technical topics to discuss.\nAfter The Offer After you get a job offer, there are a few things you can do, if you don’t like the offer you can try to negotiate, you can also ask for time to make a decision if you are waiting on other offers, or you can accept the offer right away. When trying to decide between offers, there are three main things that I look for:\nQuality of Life Role Salary Quality of life is all the little details around the job that can impact you, for example a long commute, or an expensive city, or a crappy office, etc. Often people focus on the salary alone, but if you need to commute multiple hours a day through traffic, it may not be worth the extra salary bonus. At the end of the day, just make sure that when you pick a job, that the city, commute and quality of life will be good, rather than just accepting the highest salary. Additionally, you need to take into account the Cost Of Living for the city the job is in when comparing salaries.\nThe role itself is super important too, for example you could have a very interesting role at a small company, or a boring role at a large company. Again it depends on what you want, but there is value in choosing the role over the company, as working in an interesting role at a small company will allow for very quick growth. For example, I work at kinaxis and since it is a small company, I have a ton of responsibility and am learning a lot, additionally there is a lot of room for promotion and quick career growth. On the other side, if I worker at a big company like IBM, I would have a more prestigious company name on my resume (which can help get other jobs later), but there is much less room for promotion and so I might actually progress less quickly in my career, despite being at a bigger, more well known company. As such, it’s important to not only look at the company, but also your role and opportunities for growth at the company.\nSalary is also very important. Typical salaries for new grads are from 60k-80k in Ottawa, other cities may have higher or lower salaries but you need to account for cost of living. It may be worth negotiating if you are at the lower end of the entry salary range, but for offers above 80K, I don’t imagine that there is much room to negotiate up. There are two reasons for this:\nThe way salaries work at medium and big tech companies is that there are “bands” for different levels of experience. For X experience, they will pay between Y and Z. You can negotiate within this range but can’t break outside of it without having enough experience to get to the next bracket. So if you are near the top of the entry-level bracket, you can\u0026rsquo;t negotiate up much without experience New grads often don’t have specialized industry knowledge that can be useful when negotiating salary. For example , if you have a very in demand skill, you could negotiate a higher salary since there is a shortage of software developer with that skill, but this is often not the case right out of school Conclusions In conclusion, do as many side projects, hackathons, competitions and meetups as possible to gain knowledge, connections and to polish your resume. Do this well in advance before you want to start working. Next, about 6 months before you want to start working, start looking for companies that interest you. Then spend a month or so making your resume and cover letters and sending them as soon as possible to jobs (the best time frame is 3 to 6 months before your desired start date, but can be earlier or later if needed).\nTo prepare for your interviews, review algorithms questions such as “Cracking the coding interview” as well as going over your project to refresh your memory of any interesting technical achievements.\nFinally, if you get an offer, weight the quality of life, the role itself, the possibility for growth and the salary before choosing the best option for you.\n","permalink":"https://philliams.github.io/posts/career_posts/getting_student_job/","summary":"I’ve gotten a lot of questions on how to get a job as a student. Often students want to buff up their resumes or skills a few weeks before their interviews. Honestly, I don’t think there’s much you can do in less than a few months to improve your chances. Obviously you can review algorithms, practice questions and buff up you knowledge to give you a slight advantage, but if you really want to level up and give yourself the best chance at landing the type of jobs you want, you need to start at least 6 months in advance.","title":"How to get a job as a student"},{"content":"I am a Canadian software developer and Machine Learning researcher.\n","permalink":"https://philliams.github.io/about/","summary":"I am a Canadian software developer and Machine Learning researcher.","title":"About Me"},{"content":"Online Presence Google Scholar Github Presentations 2021 Automatic Characterization of Single-Walled Carbon Nanotube Film Morphologies Using Computer Vision - Materials Research Society (MRS) Spring Symposium 2020 Machine Learning in the cloud (slides) - Canadian Undergraduate Software Engineering Conference (youtube) An introduction to time series forecasting - Guest Lecture, University of Ottawa Patents 2022 Systems And Methods For Parameter Optimization (Patent no. US11514328) - US Patent Office 2020 Analysis and correction of supply chain design through machine learning (Patent no. US10846651) - US Patent Office Research Papers 2022 Correlating Morphology, Molecular Orientation, and Transistor Performance of Bis (pentafluorophenoxy) silicon Phthalocyanine Using Scanning Transmission X-ray Microscopy - Chemistry of Materials 2021 Excess Polymer in Single-Walled Carbon Nanotube Thin-Film Transistors: Its Removal Prior to Fabrication Is Unnecessary - ACS Nano 2019 A Shallow Learning-Reduced Data Approach for Image Classification - Canadian Conference on Artificial Intelligence 2018 Time series classification with shallow learning shepard interpolation neural networks - International Conference on Image and Signal Processing Deep Convolutional-Shepard Interpolation Neural Networks for Image Classification Tasks - International Conference Image Analysis and Recognition Shepard Interpolation Neural Networks with K-Means: A Shallow Learning Method for Time Series Classification - International Joint Conference on Neural Networks 2016 SINN : Shepard Interpolation Neural Networks - International Symposium on Visual Computing ","permalink":"https://philliams.github.io/publications/","summary":"Online Presence Google Scholar Github Presentations 2021 Automatic Characterization of Single-Walled Carbon Nanotube Film Morphologies Using Computer Vision - Materials Research Society (MRS) Spring Symposium 2020 Machine Learning in the cloud (slides) - Canadian Undergraduate Software Engineering Conference (youtube) An introduction to time series forecasting - Guest Lecture, University of Ottawa Patents 2022 Systems And Methods For Parameter Optimization (Patent no. US11514328) - US Patent Office 2020 Analysis and correction of supply chain design through machine learning (Patent no.","title":"Publications"}]